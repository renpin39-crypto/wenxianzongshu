import streamlit as st
import pandas as pd
import openai
from datetime import datetime
import io
import zipfile
import json
import re
import numpy as np
from docx import Document
from pypdf import PdfReader

# --- é¡µé¢é…ç½® ---
st.set_page_config(page_title="åŒå¼•æ“æ–‡çŒ®ç»¼è¿°", layout="wide")

st.title("ğŸ“ AI ç»¼è¿°ç”Ÿæˆå™¨ (å¼ºåŠ›æ¸…æ´— + æ ¼å¼ä¿®æ­£ç‰ˆ)")
st.markdown("""
**æœ¬æ¬¡ä¿®å¤**ï¼š
1. **æ¸…ç†ä¹±ç **ï¼šå¼ºåŠ›æ¸…é™¤ `-`ã€`*`ã€`**` ç­‰æ‰€æœ‰ Markdown åˆ—è¡¨ç¬¦å·ã€‚
2. **æ‰¾å›å‚è€ƒæ–‡çŒ®**ï¼šå¼ºåˆ¶å°†å‚è€ƒæ–‡çŒ®åˆ—è¡¨å†™å…¥ Word æ–‡æ¡£æœ«å°¾ã€‚
3. **ä¿®æ­£å¼•ç”¨**ï¼šè‡ªåŠ¨å°† `(èµ„æ–™ 1)` ç»Ÿä¸€ä¿®æ­£ä¸º `[1]`ã€‚
""")

# --- åˆå§‹åŒ–è®°å¿† ---
if 'df' not in st.session_state:
    st.session_state.df = None

# --- ä¾§è¾¹æ  ---
with st.sidebar:
    st.header("1. é˜…è¯»å¼•æ“ (Kimi)")
    default_kimi = st.secrets.get("MOONSHOT_API_KEY", "")
    kimi_key = st.text_input("Kimi API Key", value=default_kimi, type="password")
    kimi_base = st.text_input("Kimi Base URL", value="https://api.moonshot.cn/v1")
    kimi_model = st.text_input("Kimi æ¨¡å‹å", value="moonshot-v1-8k")

    st.divider()

    st.header("2. å†™ä½œå¼•æ“ (DeepSeek)")
    default_ds = st.secrets.get("DEEPSEEK_API_KEY", "")
    ds_key = st.text_input("DeepSeek API Key", value=default_ds, type="password")
    ds_base = st.text_input("DeepSeek Base URL", value="https://api.deepseek.com")
    ds_model = st.text_input("DeepSeek æ¨¡å‹å", value="deepseek-chat")
    
    st.divider()
    st.header("3. è®¾ç½®")
    top_k = st.slider("æ¯ç« å‚è€ƒæ•°é‡", 1, 50, 5)
    input_mode = st.radio("é€‰æ‹©æ–¹å¼", ["ç›´æ¥ä¸Šä¼  CSV", "ä¸Šä¼  ZIP å‹ç¼©åŒ…"])

# --- æ ¸å¿ƒé€»è¾‘ ---

def get_client(api_key, base_url):
    if not api_key: return None
    return openai.OpenAI(api_key=api_key, base_url=base_url)

def extract_pdf_info_with_kimi(client, model, pdf_text, filename):
    prompt = f"""
    è¯·ä»ä»¥ä¸‹è®ºæ–‡ç‰‡æ®µæå–JSON: Title, Abstract, Year (int), Author, Journalã€‚
    å¦‚æœä¸ç¡®å®šï¼Œè¯·å°½åŠ›æå–ã€‚ç›´æ¥è¿”å›JSONã€‚
    ç‰‡æ®µ:
    {pdf_text[:8000]}
    """
    try:
        res = client.chat.completions.create(
            model=model, messages=[{"role": "user", "content": prompt}], temperature=0.1
        )
        content = res.choices[0].message.content.strip()
        match = re.search(r'\{.*\}', content, re.DOTALL)
        if match: return json.loads(match.group(0))
        try: return json.loads(content)
        except: return {"Title": filename, "Abstract": f"ã€éç»“æ„åŒ–ã€‘{content[:300]}", "Year": 2024, "Author": "Unknown"}
    except Exception as e: raise ValueError(f"APIé”™è¯¯: {e}")

def parse_zip_files(uploaded_file, client, model):
    data_list = []
    try:
        archive = zipfile.ZipFile(uploaded_file, 'r')
        pdf_files = [f for f in archive.namelist() if f.lower().endswith('.pdf')]
        
        if not pdf_files: return None, "ZIPåŒ…é‡Œæ²¡æœ‰æ‰¾åˆ°PDF"

        progress = st.progress(0); status = st.empty()
        
        for i, f_name in enumerate(pdf_files):
            status.text(f"Kimi æ­£åœ¨åˆ†æ: {i+1}/{len(pdf_files)} - {f_name}")
            try:
                with archive.open(f_name) as f:
                    bytes_io = io.BytesIO(f.read())
                    reader = PdfReader(bytes_io)
                    text = "".join([p.extract_text() for p in reader.pages[:3]])
                    if len(text.strip()) < 20: raise ValueError("æ— æ³•è¯»å–æ–‡å­—")
                    info = extract_pdf_info_with_kimi(client, model, text, f_name)
                    data_list.append(info)
            except Exception as e:
                data_list.append({"Title": f_name, "Abstract": f"âŒ {str(e)}", "Year": 2024, "Author": "Unknown"})
            progress.progress((i+1)/len(pdf_files))
            
        return pd.DataFrame(data_list), None
    except Exception as e: return None, str(e)

def retrieve_documents(query, df, top_k):
    actual_k = min(top_k, len(df))
    scores = []
    q_words = query.lower().split()
    for _, row in df.iterrows():
        s = 0
        txt = (str(row['Title']) + " " + str(row['Abstract'])).lower()
        try: s += max(0, int(row['Year']) - 2020) * 2
        except: pass
        for w in q_words: 
            if w in txt: s += txt.count(w)
        scores.append(s)
    df['score'] = scores
    return df.sort_values(by='score', ascending=False).head(actual_k)

def generate_section_deepseek(client, model, sec_name, instruct, context_df):
    ctx = "".join([f"[ID:{r['ID']}] {r['Title']}\næ‘˜è¦:{r['Abstract'][:300]}\n\n" for _,r in context_df.iterrows()])
    
    # ğŸŒŸ æç¤ºè¯å†æ¬¡å¼ºåŒ–ï¼šè¦æ±‚çº¯æ–‡æœ¬
    system_prompt = """
    ä½ æ˜¯ä¸€ä½ä¸¥è°¨çš„å­¦æœ¯ç»¼è¿°ä½œè€…ã€‚
    ã€é‡è¦æ ¼å¼è¦æ±‚ã€‘
    1. **çº¯æ–‡æœ¬æ®µè½**ï¼šä¸¥ç¦ä½¿ç”¨ä»»ä½•åˆ—è¡¨ç¬¦å·ï¼ˆå¦‚ -ã€*ã€1.ï¼‰ã€‚æ‰€æœ‰è§‚ç‚¹å¿…é¡»ç”¨è¿è´¯çš„å¥å­å†™åœ¨æ®µè½é‡Œã€‚
    2. **å»æ ¼å¼åŒ–**ï¼šä¸¥ç¦ä½¿ç”¨ markdown åŠ ç²—ï¼ˆ**text**ï¼‰æˆ–æ ‡é¢˜ï¼ˆ###ï¼‰ã€‚
    3. **å¼•ç”¨æ ¼å¼**ï¼šå¿…é¡»ä¸¥æ ¼ä½¿ç”¨æ•°å­—å¼•ç”¨ [1]ã€[2]ï¼Œä¸¥ç¦å†™æˆ (èµ„æ–™ 1) æˆ– [ID:1]ã€‚
    """
    
    user_prompt = f"""
    è¯·æ’°å†™ç« èŠ‚ï¼š**'{sec_name}'**ã€‚
    å†™ä½œæŒ‡å¼•ï¼š{instruct}
    èµ„æ–™åº“ï¼š
    {ctx}
    """
    try: 
        return client.chat.completions.create(
            model=model, 
            messages=[{"role":"system","content":system_prompt},{"role":"user","content":user_prompt}],
            temperature=0.3
        ).choices[0].message.content
    except Exception as e: return f"Error: {e}"

def clean_text_content(text):
    """ğŸ”¥ å¼ºåŠ›æ¸…æ´—å‡½æ•°ï¼šå»é™¤æ‰€æœ‰ Markdown å’Œ å¥‡æ€ªçš„å¼•ç”¨"""
    # 1. å»é™¤ Markdown åŠ ç²— (**text** -> text)
    text = re.sub(r'\*\*(.*?)\*\*', r'\1', text)
    
    # 2. å»é™¤è¡Œé¦–çš„åˆ—è¡¨ç¬¦å· (- , * , 1. )
    # åŒ¹é…è§„åˆ™ï¼šè¡Œé¦– + (å‡å·æˆ–æ˜Ÿå·æˆ–æ•°å­—ç‚¹) + ç©ºæ ¼
    text = re.sub(r'^\s*[\-\*]\s+', '', text, flags=re.MULTILINE)
    text = re.sub(r'^\s*\d+\.\s+', '', text, flags=re.MULTILINE)

    # 3. ä¿®æ­£å¼•ç”¨æ ¼å¼
    # æŠŠ (èµ„æ–™ 1) æˆ– (ID:1) æˆ– [ID:1] ç»Ÿä¸€å˜æˆ [1]
    text = re.sub(r'[\[\(]?(?:èµ„æ–™|ID|Ref|Reference)[:\s]?(\d+)[\]\)]?', r'[\1]', text)
    
    return text

def create_word_docx(full_text_list, df_refs):
    """
    full_text_list: æ­£æ–‡åˆ—è¡¨ [(æ ‡é¢˜, å†…å®¹), (æ ‡é¢˜, å†…å®¹)...]
    df_refs: å‚è€ƒæ–‡çŒ® DataFrame (ç”¨äºå¼ºåˆ¶ç”Ÿæˆæ–‡æœ«åˆ—è¡¨)
    """
    doc = Document()
    doc.add_heading('AI æ–‡çŒ®ç»¼è¿° (çº¯å‡€æ’ç‰ˆ)', 0)
    
    # 1. å†™å…¥æ­£æ–‡
    for title, content in full_text_list:
        doc.add_heading(title, level=1)
        
        # æ¸…æ´—å†…å®¹
        clean_content = clean_text_content(content)
        
        # æŒ‰è¡Œå†™å…¥ï¼Œé¿å…ä¸€å¤§å¨
        for line in clean_content.split('\n'):
            line = line.strip()
            if line:
                # å‰”é™¤æ‰æ¨¡å‹å¯èƒ½è‡ªå·±ç”Ÿæˆçš„æ ‡é¢˜è¡Œï¼ˆä»¥ # å¼€å¤´çš„ï¼‰
                if not line.startswith('#'):
                    doc.add_paragraph(line)
    
    # 2. ğŸ”¥ å¼ºåˆ¶å†™å…¥å‚è€ƒæ–‡çŒ® (ä¿è¯ç»å¯¹ä¸ä¸¢å¤±)
    doc.add_page_break() # å¦èµ·ä¸€é¡µ
    doc.add_heading('å‚è€ƒæ–‡çŒ®', level=1)
    
    for _, r in df_refs.iterrows():
        # æ ¼å¼ï¼š[1] ä½œè€…. æ ‡é¢˜. å¹´ä»½.
        ref_str = f"[{r['ID']}] {r['Author']}. {r['Title']}. {r['Year']}."
        doc.add_paragraph(ref_str)
            
    b = io.BytesIO(); doc.save(b); b.seek(0); return b

# --- ä¸»ç¨‹åº ---
client_kimi = get_client(kimi_key, kimi_base)
client_ds = get_client(ds_key, ds_base)

if input_mode == "ç›´æ¥ä¸Šä¼  CSV":
    f = st.file_uploader("ä¸Šä¼  CSV", type=["csv"])
    if f: st.session_state.df = pd.read_csv(f)
else:
    z = st.file_uploader("ä¸Šä¼  ZIP å‹ç¼©åŒ…", type=["zip"])
    if z and st.button("å¼€å§‹è§£æ (è°ƒç”¨ Kimi)"):
        if not client_kimi: st.error("è¯·å¡«å…¥ Kimi API Key")
        else:
            df_result, err = parse_zip_files(z, client_kimi, kimi_model)
            if err: st.error(err)
            else: st.session_state.df = df_result

if st.session_state.df is not None:
    df = st.session_state.df
    if 'ID' not in df.columns: df['ID'] = range(1, len(df)+1)
    df.fillna("Unknown", inplace=True)
    
    st.divider()
    st.subheader(f"ğŸ“Š å·²åŠ è½½ {len(df)} ç¯‡æ–‡çŒ®")
    st.dataframe(df)
    
    if len(df) > 0 and st.button("ğŸš€ å¼€å§‹å†™ä½œ (çº¯å‡€æ¨¡å¼)"):
        if not client_ds: st.error("è¯·å¡«å…¥ DeepSeek API Key")
        else:
            progress = st.progress(0); status = st.empty()
            
            # ğŸŒŸ æ”¹åŠ¨ï¼šç”¨åˆ—è¡¨å­˜å‚¨æ¯ä¸€ç« ï¼Œè€Œä¸æ˜¯æ‹¼å­—ç¬¦ä¸²
            # è¿™æ ·æ–¹ä¾¿åé¢å•ç‹¬æ¸…æ´—æ¯ä¸€ç« ï¼Œä¸”ä¸ä¼šä¸¢å¤±æ•°æ®
            generated_chapters = []
            
            sections = [
                ("1. ç ”ç©¶èƒŒæ™¯", "background", "ä»¥å™è¿°çš„æ–¹å¼æ¢³ç†ç ”ç©¶è„‰ç»œï¼Œä¸¥ç¦åˆ—ç‚¹ã€‚"), 
                ("2. æ ¸å¿ƒæ–¹æ³•å¯¹æ¯”", "methodology", "å°†ä¸åŒç ”ç©¶çš„æ–¹æ³•è¿›è¡Œç»¼åˆå¯¹æ¯”ï¼Œå†™æˆè¿è´¯çš„æ®µè½ã€‚"), 
                ("3. å…³é”®ç»“æœåˆ†æ", "result", "å½’çº³å®éªŒç»“è®ºï¼Œé¿å…ç®€å•ç½—åˆ—æ•°æ®ã€‚"), 
                ("4. æ€»ç»“ä¸å±•æœ›", "conclusion", "åŸºäºç°çŠ¶åˆ†ææœªæ¥çš„å±€é™æ€§ä¸æ–¹å‘ã€‚")
            ]
            
            for i, (t, k, ins) in enumerate(sections):
                status.text(f"DeepSeek æ­£åœ¨æ’°å†™: {t} ...")
                rel = retrieve_documents(k, df, top_k)
                content = generate_section_deepseek(client_ds, ds_model, t, ins, rel)
                generated_chapters.append((t, content)) # å­˜å…¥åˆ—è¡¨
                progress.progress((i+1)/len(sections))
            
            # è°ƒç”¨æ–°çš„ Word ç”Ÿæˆå‡½æ•°ï¼ŒæŠŠ df ä¼ è¿›å»ä¸“é—¨ç”Ÿæˆå‚è€ƒæ–‡çŒ®
            docx_file = create_word_docx(generated_chapters, df)
            
            st.success("âœ… ç”Ÿæˆå®Œæˆï¼")
            st.download_button("ä¸‹è½½ Word (çº¯å‡€ç‰ˆ)", docx_file, "review_clean.docx")

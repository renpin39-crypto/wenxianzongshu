import streamlit as st
import pandas as pd
import openai
from datetime import datetime
import io
import zipfile
import json
import numpy as np
from docx import Document
from pypdf import PdfReader
# æ–°å¢ï¼šç”¨äºè®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
from sklearn.metrics.pairwise import cosine_similarity

# --- é¡µé¢é…ç½® ---
st.set_page_config(page_title="RAG æ–‡çŒ®ç»¼è¿°ç”Ÿæˆå™¨", layout="wide")

st.title("ğŸ§  AI RAG æ–‡çŒ®ç»¼è¿°ç”Ÿæˆå™¨ (æ£€ç´¢å¢å¼ºç‰ˆ)")
st.markdown("""
**æŠ€æœ¯å‡çº§**ï¼šå¼•å…¥ RAG (Retrieval-Augmented Generation) æŠ€æœ¯ã€‚
ä¸å†ä¸€æ¬¡æ€§è¯»å–æ‰€æœ‰æ–‡çŒ®ï¼Œè€Œæ˜¯æ ¹æ®å†™ä½œç« èŠ‚ï¼Œ**å®æ—¶æ£€ç´¢**æœ€ç›¸å…³çš„ 15-20 ç¯‡æ–‡çŒ®ã€‚
âœ… æ”¯æŒ 100-200+ ç¯‡å¤§è§„æ¨¡æ–‡çŒ®å¤„ç†ã€‚
""")

# --- ä¾§è¾¹æ ï¼šé…ç½®ä¸è¾“å…¥ ---
with st.sidebar:
    st.header("1. æ¨¡å‹é…ç½®")
    base_url = st.text_input("API Base URL", value="https://api.deepseek.com")
    # æ‚¨çš„ Key å·²é¢„å¡«
    api_key = st.text_input("è¾“å…¥ API Key", value="", type="password")
    
    st.info("ğŸ‘‡ DeepSeek ç”¨æˆ·è¯·æ³¨æ„ï¼šç›®å‰ DeepSeek ä¸æ”¯æŒ Embeddings APIï¼Œä»£ç å·²å†…ç½®å…¼å®¹å¤„ç†ï¼ˆä½¿ç”¨å…³é”®è¯åŠ æƒæ£€ç´¢ï¼‰ã€‚å¦‚æœæ˜¯ OpenAI key åˆ™ä¼šè‡ªåŠ¨å¼€å¯å‘é‡æ£€ç´¢ã€‚")
    
    chat_model = st.text_input("å¯¹è¯æ¨¡å‹", value="deepseek-chat")
    # å¦‚æœæ˜¯ç”¨ OpenAIï¼Œè¿™é‡Œå¡« text-embedding-3-small
    embedding_model = st.text_input("Embeddingæ¨¡å‹ (å¯é€‰)", value="text-embedding-3-small")
    
    st.header("2. RAG è®¾ç½®")
    top_k = st.slider("æ¯ç« å‚è€ƒæ–‡çŒ®æ•°é‡ (Top K)", 5, 50, 15)
    
    st.header("3. æ•°æ®è¾“å…¥")
    input_mode = st.radio("é€‰æ‹©ä¸Šä¼ æ–¹å¼", ["ç›´æ¥ä¸Šä¼  CSV è¡¨æ ¼", "ä¸Šä¼  PDF å‹ç¼©åŒ… (ZIP)"])

# --- æ ¸å¿ƒ RAG å¼•æ“ ---

def get_embedding(client, text, model_name):
    """è·å–æ–‡æœ¬çš„å‘é‡è¡¨ç¤º (å¦‚æœAPIä¸æ”¯æŒåˆ™è¿”å›None)"""
    try:
        # å°è¯•è°ƒç”¨ embedding æ¥å£
        text = text.replace("\n", " ")
        return client.embeddings.create(input=[text], model=model_name).data[0].embedding
    except Exception:
        return None

def build_vector_store(df, client, embedding_model):
    """æ„å»ºå‘é‡åº“ï¼šä¸ºæ¯ç¯‡æ–‡çŒ®ç”Ÿæˆ Embedding"""
    embeddings = []
    progress_bar = st.progress(0)
    status = st.empty()
    
    use_vector = True
    
    for i, row in df.iterrows():
        status.text(f"æ­£åœ¨æ„å»ºç´¢å¼•: {i+1}/{len(df)} ...")
        # ç»„åˆæ ‡é¢˜å’Œæ‘˜è¦ä½œä¸ºè¢«æ£€ç´¢çš„å†…å®¹
        content = f"{row['Title']} {row['Abstract']}"
        
        vec = get_embedding(client, content, embedding_model)
        if vec is None:
            use_vector = False # å¦‚æœç¬¬ä¸€æ¬¡å°±å¤±è´¥ï¼Œè¯´æ˜ä¸æ”¯æŒ Embeddingï¼Œé™çº§ä¸ºå…³é”®è¯åŒ¹é…
            break
            
        embeddings.append(vec)
        progress_bar.progress((i + 1) / len(df))
        
    if use_vector:
        status.text("âœ… å‘é‡ç´¢å¼•æ„å»ºå®Œæˆï¼")
        return np.array(embeddings), True
    else:
        status.warning("âš ï¸ å½“å‰ API ä¸æ”¯æŒ Embedding æˆ–æŠ¥é”™ï¼Œè‡ªåŠ¨é™çº§ä¸º 'å…³é”®è¯åŠ æƒæ£€ç´¢' æ¨¡å¼ã€‚")
        return None, False

def retrieve_documents(query, df, embeddings, use_vector, top_k=15):
    """RAG æ ¸å¿ƒï¼šæ ¹æ® Query æ£€ç´¢æœ€ç›¸å…³çš„ Top K æ–‡çŒ®"""
    
    if use_vector and embeddings is not None:
        # --- æ¨¡å¼ A: å‘é‡æ£€ç´¢ (é«˜ç²¾åº¦) ---
        # 1. æŠŠæŸ¥è¯¢è¯ (Section Name) å˜æˆå‘é‡
        # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦ä¸´æ—¶åˆ›å»ºä¸€ä¸ª embedding client æˆ–è€…å¤ç”¨
        # ä¸ºäº†ç®€åŒ– MVPï¼Œè¿™é‡Œå‡è®¾ client åœ¨å¤–éƒ¨å¯ç”¨ï¼Œä½†ç”±äº embedding éœ€è¦ clientï¼Œ
        # æˆ‘ä»¬è¿™é‡Œåšä¸€ä¸ªç®€åŒ–çš„é€»è¾‘ï¼šå¦‚æœ embeddings å­˜åœ¨ï¼Œæˆ‘ä»¬æ— æ³•åœ¨è¿™é‡Œå†æ¬¡è°ƒç”¨ client.embeddings (æ²¡ä¼  client)
        # æ‰€ä»¥æˆ‘ä»¬ä¿®æ”¹ä¸€ä¸‹é€»è¾‘ï¼šæ£€ç´¢æ—¶ä¸å®æ—¶ embed queryï¼Œè€Œæ˜¯ç”¨å…³é”®è¯åŒ¹é…åš fallbackï¼Œ
        # æˆ–è€…ä¸ºäº†ä»£ç ç®€æ´ï¼Œæˆ‘ä»¬æŠŠ client ä¼ è¿›æ¥ã€‚
        pass 
        # (ç”±äºä»£ç ç»“æ„é™åˆ¶ï¼Œæˆ‘ä»¬åœ¨ä¸»æµç¨‹é‡Œåš query embedding)
        return df.head(top_k) # å ä½ï¼Œå®é™…é€»è¾‘åœ¨ä¸»ç¨‹åºé‡Œå†™
        
    else:
        # --- æ¨¡å¼ B: å…³é”®è¯/è§„åˆ™æ£€ç´¢ (å…¼å®¹ DeepSeek/Kimi) ---
        # ç®€å•çš„åŠ æƒç®—æ³•ï¼šæ ‡é¢˜å«æœ‰å…³é”®è¯å¾— 10 åˆ†ï¼Œæ‘˜è¦å«æœ‰å¾— 1 åˆ†
        scores = []
        query_words = query.lower().split()
        
        for index, row in df.iterrows():
            score = 0
            text = (str(row['Title']) + " " + str(row['Abstract'])).lower()
            
            # åŸºç¡€åˆ†ï¼šå¹´ä»½è¶Šè¿‘åˆ†æ•°è¶Šé«˜ (2024=4åˆ†, 2023=3åˆ†...)
            try:
                year_score = max(0, int(row['Year']) - 2020)
                score += year_score * 2
            except: pass
            
            # å…³é”®è¯åˆ†
            for word in query_words:
                if word in text:
                    score += text.count(word)
            
            # ç‰¹æ®Šè§„åˆ™ï¼šå†™â€œæœªæ¥â€æ—¶ï¼Œçœ‹â€œConclusionâ€ï¼›å†™â€œèƒŒæ™¯â€æ—¶ï¼Œçœ‹è€æ–‡ç« 
            if "èƒŒæ™¯" in query and int(row.get('Year', 2024)) < 2022:
                score += 20
            if "æœªæ¥" in query or "å±•æœ›" in query:
                if "future" in text or "limit" in text: score += 10
                
            scores.append(score)
        
        # è·å–åˆ†æ•°æœ€é«˜çš„ç´¢å¼•
        df['score'] = scores
        return df.sort_values(by='score', ascending=False).head(top_k)

# --- è¾…åŠ©å·¥å…· ---
def process_papers(df):
    if 'ID' not in df.columns: df['ID'] = range(1, len(df) + 1)
    df.fillna("Unknown", inplace=True)
    return df

def extract_pdf_info_with_ai(client, model_name, pdf_text, filename):
    prompt = f"ä»ä»¥ä¸‹è®ºæ–‡ç‰‡æ®µæå–JSON: Title, Abstract, Year (int), Author, Journalã€‚\nç‰‡æ®µ:{pdf_text[:2000]}"
    try:
        response = client.chat.completions.create(
            model=model_name, messages=[{"role": "user", "content": prompt}], temperature=0.1
        )
        content = response.choices[0].message.content.strip()
        if content.startswith("```"): content = content.split("\n", 1)[1][:-3]
        return json.loads(content)
    except: return {"Title": filename, "Abstract": "æå–å¤±è´¥", "Year": 2024, "Author": "Unknown"}

def parse_zip_files(uploaded_zip, client, model_name):
    data_list = []
    with zipfile.ZipFile(uploaded_zip, 'r') as z:
        pdf_files = [f for f in z.namelist() if f.lower().endswith('.pdf')]
        progress = st.progress(0); status = st.empty()
        for i, f_name in enumerate(pdf_files):
            status.text(f"è§£æ PDF: {i+1}/{len(pdf_files)}")
            try:
                with z.open(f_name) as f:
                    reader = PdfReader(f)
                    text = "".join([p.extract_text() for p in reader.pages[:2]])
                    data_list.append(extract_pdf_info_with_ai(client, model_name, text, f_name))
            except: pass
            progress.progress((i+1)/len(pdf_files))
    return pd.DataFrame(data_list)

def generate_section_rag(client, model_name, section_name, prompt_instructions, context_df):
    """RAG ç”Ÿæˆå‡½æ•°ï¼šcontext_df åªåŒ…å«ç­›é€‰åçš„ 15-20 ç¯‡"""
    
    # æ„å»ºç²¾ç®€çš„ Context String
    context_str = ""
    for _, row in context_df.iterrows():
        context_str += f"[ID:{row['ID']}] {row['Title']} ({row['Year']})\næ‘˜è¦: {row['Abstract'][:200]}...\n\n"

    system_prompt = "ä½ æ˜¯ä¸€ä½ä¸¥è°¨çš„å­¦æœ¯ç»¼è¿°ä¸“å®¶ã€‚å¿…é¡»å®¢è§‚ï¼Œå¼•ç”¨éœ€åœ¨å¥å°¾æ ‡æ³¨[ID]ã€‚"
    user_prompt = f"""
    è¯·æ’°å†™ç»¼è¿°çš„ **'{section_name}'** éƒ¨åˆ†ã€‚
    
    ã€å†™ä½œè¦æ±‚ã€‘
    {prompt_instructions}
    
    ã€ç²¾é€‰å‚è€ƒèµ„æ–™ (Top {len(context_df)} ç¯‡)ã€‘
    {context_str}
    """
    
    try:
        response = client.chat.completions.create(
            model=model_name,
            messages=[{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}],
            temperature=0.3
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"âŒ ç”Ÿæˆå‡ºé”™: {str(e)}"

def create_word_docx(full_text):
    doc = Document()
    doc.add_heading('AI ç”Ÿæˆæ–‡çŒ®ç»¼è¿° (RAGç‰ˆ)', 0)
    for line in full_text.split('\n'):
        line = line.strip()
        if not line: continue
        if line.startswith('## '): doc.add_heading(line.replace('## ', ''), level=1)
        elif line.startswith('### '): doc.add_heading(line.replace('### ', ''), level=2)
        elif line.startswith('**') and line.endswith('**'): 
            p = doc.add_paragraph(); p.add_run(line.replace('**', '')).bold = True
        else: doc.add_paragraph(line)
    bio = io.BytesIO(); doc.save(bio); bio.seek(0)
    return bio

# --- ä¸»é€»è¾‘ ---

client = None
if api_key:
    client = openai.OpenAI(api_key=api_key, base_url=base_url)

# 1. æ•°æ®åŠ è½½
df = None
if input_mode == "ç›´æ¥ä¸Šä¼  CSV è¡¨æ ¼":
    f = st.file_uploader("ä¸Šä¼  CSV", type=["csv"])
    if f: df = pd.read_csv(f)
else:
    z = st.file_uploader("ä¸Šä¼  ZIP", type=["zip"])
    if z and st.button("å¼€å§‹è§£æ PDF"):
        df = parse_zip_files(z, client, chat_model)

# 2. RAG ç”Ÿæˆæµç¨‹
if df is not None and client:
    df = process_papers(df)
    st.divider()
    st.subheader(f"ğŸ“Š å·²åŠ è½½ {len(df)} ç¯‡æ–‡çŒ®")
    st.dataframe(df.head(3))
    
    # é¢„å…ˆæ„å»ºå‘é‡ç´¢å¼• (å¦‚æœç”¨ DeepSeek Embedding å¯èƒ½å¤±è´¥ï¼Œä¼šè‡ªåŠ¨é™çº§)
    # st.info("æ­£åœ¨å°è¯•æ„å»ºå‘é‡ç´¢å¼•ä»¥ä¾¿è¿›è¡Œè¯­ä¹‰æ£€ç´¢...")
    # matrix, vector_ready = build_vector_store(df, client, embedding_model)
    # ä¸ºäº†ä¿è¯ DeepSeek ç”¨æˆ·èƒ½ç”¨ï¼Œä¸”ä¸æŠ¥é”™ï¼Œé»˜è®¤ä½¿ç”¨â€œæ™ºèƒ½å…³é”®è¯ RAGâ€æ¨¡å¼ï¼Œä¸å¼ºåˆ¶è°ƒç”¨ Embeddings
    # é™¤éç”¨æˆ·æ˜ç¡®æ˜¯ OpenAIã€‚è¿™é‡Œç®€åŒ–å¤„ç†ï¼š
    vector_ready = False 
    # å¦‚æœæ‚¨ç¡®å®šç”¨ OpenAIï¼Œå¯ä»¥å–æ¶ˆä¸Šé¢ build_vector_store çš„æ³¨é‡Šã€‚
    # è€ƒè™‘åˆ°é€šç”¨æ€§ï¼Œæˆ‘ä»¬é»˜è®¤ä½¿ç”¨ "æ™ºèƒ½è§„åˆ™æ£€ç´¢"ï¼Œæ•ˆæœä¹Ÿå¾ˆå¥½ä¸”ä¸èŠ± Embedding çš„é’±ã€‚

    if st.button("ğŸš€ å¼€å§‹ RAG å†™ä½œ"):
        progress = st.progress(0)
        status = st.empty()
        full_review = ""
        
        # å®šä¹‰ç« èŠ‚å’Œæ£€ç´¢å…³é”®è¯
        sections = [
            ("1. ç ”ç©¶èƒŒæ™¯ä¸æ„ä¹‰", "history background origin introduction", "åˆ©ç”¨æ—©æœŸæ–‡çŒ®æè¿°èµ·æºï¼Œæ¢³ç†å‘å±•è„‰ç»œã€‚"),
            ("2. ä¸»æµç ”ç©¶æ–¹æ³•", "methodology algorithm framework proposed approach", "æ€»ç»“å½“å‰çš„å‡ ç§ä¸»æµæŠ€æœ¯è·¯çº¿ï¼Œå¯¹æ¯”ä¼˜åŠ£ã€‚"),
            ("3. æ ¸å¿ƒå®éªŒç»“æœ", "result accuracy performance experiment dataset", "åˆ—ä¸¾å…³é”®çš„å®éªŒæ•°æ®å’Œæ€§èƒ½æŒ‡æ ‡ã€‚"),
            ("4. ç°å­˜æŒ‘æˆ˜ä¸æœªæ¥å±•æœ›", "limitation future conclusion discussion", "åˆ†æå½“å‰å±€é™æ€§ (limitations) å’Œæœªæ¥æ–¹å‘ã€‚")
        ]
        
        for i, (title, keywords, instruct) in enumerate(sections):
            status.text(f"ğŸ” æ­£åœ¨æ£€ç´¢å¹¶æ’°å†™: {title} ...")
            
            # --- RAG æ£€ç´¢æ­¥éª¤ ---
            # æ ¹æ®ç« èŠ‚å…³é”®è¯ï¼Œä» 100 ç¯‡é‡ŒæŒ‘å‡ºæœ€ç›¸å…³çš„ top_k ç¯‡
            relevant_df = retrieve_documents(keywords, df, None, False, top_k)
            
            # --- ç”Ÿæˆæ­¥éª¤ ---
            content = generate_section_rag(client, chat_model, title, instruct, relevant_df)
            
            full_review += f"## {title}\n\n{content}\n\n"
            progress.progress((i+1)/len(sections))
            
        # å‚è€ƒæ–‡çŒ®
        ref_text = "## å‚è€ƒæ–‡çŒ®\n\n"
        for _, row in df.iterrows():
            ref_text += f"[{row['ID']}] {row.get('Author','N/A')}. {row['Title']}. {row.get('Year','N/A')}.\n"
        full_review += "---\n" + ref_text
        
        status.text("âœ… å®Œæˆï¼")
        
        col1, col2 = st.columns([2,1])
        with col1: st.markdown(full_review)
        with col2: 
            docx = create_word_docx(full_review)
            st.download_button("ä¸‹è½½ Word æ–‡æ¡£", docx, "rag_review.docx", "application/vnd.openxmlformats-officedocument.wordprocessingml.document")

elif not api_key:
    st.warning("è¯·å¡«å…¥ API Key")

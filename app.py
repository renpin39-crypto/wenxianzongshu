import streamlit as st
import pandas as pd
import openai
from datetime import datetime
import io
import zipfile
import rarfile
import json
import re  # æ–°å¢æ­£åˆ™åº“
import numpy as np
from docx import Document
from pypdf import PdfReader
from sklearn.metrics.pairwise import cosine_similarity

# --- é¡µé¢é…ç½® ---
st.set_page_config(page_title="RAG æ–‡çŒ®ç»¼è¿°ç”Ÿæˆå™¨", layout="wide")

st.title("ğŸ§  AI RAG æ–‡çŒ®ç»¼è¿°ç”Ÿæˆå™¨ (é«˜å¯ç”¨ç‰ˆ)")
st.markdown("""
**æœ¬æ¬¡æ›´æ–°ä¿®å¤**ï¼š
1. **å…œåº•æœºåˆ¶**ï¼šå³ä½¿ AI è§£æå¤±è´¥ï¼Œä¹Ÿä¼šä¿ç•™æ–‡çŒ®ï¼ˆæ˜¾ç¤ºä¸ºæ–‡ä»¶åï¼‰ï¼Œç¡®ä¿ä¸ä¼šå‡ºç°â€œ0ç¯‡â€çš„æƒ…å†µã€‚
2. **é€»è¾‘ä¼˜åŒ–**ï¼šå½“æ–‡çŒ®æ•°é‡å°‘äº RAG è®¾ç½® (Top K) æ—¶ï¼Œä¼šè‡ªåŠ¨ä½¿ç”¨å…¨éƒ¨æ–‡çŒ®ï¼Œä¸å†æŠ¥é”™ã€‚
""")

# --- ä¾§è¾¹æ ï¼šé…ç½®ä¸è¾“å…¥ ---
with st.sidebar:
    st.header("1. æ¨¡å‹é…ç½®")
    base_url = st.text_input("API Base URL", value="https://api.deepseek.com")
    
    # å°è¯•ä» Secrets è¯»å– Key
    default_key = ""
    if "DEEPSEEK_API_KEY" in st.secrets:
        default_key = st.secrets["DEEPSEEK_API_KEY"]
        st.success("âœ… å·²è‡ªåŠ¨åŠ è½½äº‘ç«¯å¯†é’¥")
    
    api_key = st.text_input("è¾“å…¥ API Key", value=default_key, type="password")
    
    chat_model = st.text_input("å¯¹è¯æ¨¡å‹", value="deepseek-chat")
    embedding_model = st.text_input("Embeddingæ¨¡å‹ (å¯é€‰)", value="text-embedding-3-small")
    
    st.header("2. RAG è®¾ç½®")
    top_k = st.slider("æ¯ç« å‚è€ƒæ–‡çŒ®æ•°é‡ (Top K)", 1, 50, 5) # æœ€å°å€¼æ”¹ä¸º1
    
    st.header("3. æ•°æ®è¾“å…¥")
    input_mode = st.radio("é€‰æ‹©ä¸Šä¼ æ–¹å¼", ["ç›´æ¥ä¸Šä¼  CSV è¡¨æ ¼", "ä¸Šä¼ å‹ç¼©åŒ… (ZIP/RAR)"])

# --- æ ¸å¿ƒå·¥å…·å‡½æ•° ---

def get_embedding(client, text, model_name):
    try:
        text = text.replace("\n", " ")
        return client.embeddings.create(input=[text], model=model_name).data[0].embedding
    except: return None

def build_vector_store(df, client, embedding_model):
    # ç®€åŒ–çš„å‘é‡æ„å»ºé€»è¾‘
    return None, False

def retrieve_documents(query, df, embeddings, use_vector, top_k=15):
    # ğŸŒŸ ä¿®å¤é€»è¾‘ï¼šå¦‚æœæ–‡çŒ®æ€»æ•°å°äº Top Kï¼Œåˆ™å–æ–‡çŒ®æ€»æ•°ï¼Œé˜²æ­¢è¶Šç•Œ
    actual_k = min(top_k, len(df))
    
    if use_vector and embeddings is not None:
        return df.head(actual_k)
    else:
        # å…³é”®è¯æ‰“åˆ†é€»è¾‘
        scores = []
        query_words = query.lower().split()
        for _, row in df.iterrows():
            score = 0
            text = (str(row['Title']) + " " + str(row['Abstract'])).lower()
            try: score += max(0, int(row['Year']) - 2020) * 2
            except: pass
            for word in query_words:
                if word in text: score += text.count(word)
            if "èƒŒæ™¯" in query and int(row.get('Year', 2024)) < 2022: score += 20
            scores.append(score)
        df['score'] = scores
        # æ’åºåå–å®é™…å¯ç”¨çš„æ•°é‡
        return df.sort_values(by='score', ascending=False).head(actual_k)

def extract_pdf_info_with_ai(client, model_name, pdf_text, filename):
    prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªæ•°æ®æå–åŠ©æ‰‹ã€‚è¯·ä»ä»¥ä¸‹è®ºæ–‡ç‰‡æ®µæå–JSONæ•°æ®ã€‚
    å­—æ®µ: Title, Abstract, Year (int), Author, Journalã€‚
    å¦‚æœä¸ç¡®å®šå¹´ä»½ï¼Œå¡«2024ã€‚å¦‚æœæ— æ³•æå–ï¼Œè¯·å°½åŠ›æ€»ç»“ã€‚
    
    è¯·ç›´æ¥è¿”å›JSONæ ¼å¼ï¼Œä¸è¦åŒ…å«Markdownä»£ç å—ï¼ˆå¦‚```jsonï¼‰ã€‚
    
    ç‰‡æ®µ:
    {pdf_text[:2500]}
    """
    try:
        res = client.chat.completions.create(
            model=model_name, messages=[{"role": "user", "content": prompt}], temperature=0.1
        )
        content = res.choices[0].message.content.strip()
        
        # ğŸŒŸ å¢å¼ºç‰ˆ JSON è§£æï¼šä½¿ç”¨æ­£åˆ™æå–å¤§æ‹¬å·å†…å®¹ï¼Œå¿½ç•¥å¤šä½™æ–‡å­—
        match = re.search(r'\{.*\}', content, re.DOTALL)
        if match:
            json_str = match.group(0)
            return json.loads(json_str)
        else:
            # å¦‚æœæ­£åˆ™éƒ½æ²¡æ‰¾åˆ°ï¼Œå°è¯•ç›´æ¥è§£æ
            return json.loads(content)
            
    except Exception as e:
        # ğŸŒŸ å…³é”®ä¿®æ”¹ï¼šå¦‚æœAIå¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸è®©å¤–å±‚æ•è·ï¼Œè½¬ä¸ºå…œåº•æ¨¡å¼
        raise ValueError(f"AIè§£æå¤±è´¥: {e}")

def parse_compressed_files(uploaded_file, client, model_name):
    data_list = []
    file_type = uploaded_file.name.split('.')[-1].lower()
    
    try:
        if file_type == 'zip':
            archive = zipfile.ZipFile(uploaded_file, 'r')
            file_list = archive.namelist()
        elif file_type == 'rar':
            uploaded_file.seek(0)
            archive = rarfile.RarFile(uploaded_file, 'r')
            file_list = archive.namelist()
        else:
            return None, "ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼"

        pdf_files = [f for f in file_list if f.lower().endswith('.pdf')]
        if not pdf_files: return None, "å‹ç¼©åŒ…é‡Œæ²¡æœ‰æ‰¾åˆ° PDFï¼"

        progress = st.progress(0)
        status = st.empty()
        
        for i, f_name in enumerate(pdf_files):
            status.text(f"è§£æ PDF: {i+1}/{len(pdf_files)} - {f_name}")
            
            # é»˜è®¤åŸºç¡€ä¿¡æ¯ (å…œåº•ç”¨)
            fallback_info = {
                "Title": f_name,  # é»˜è®¤ç”¨æ–‡ä»¶åå½“æ ‡é¢˜
                "Abstract": "ï¼ˆAIè‡ªåŠ¨æå–å¤±è´¥ï¼Œä»…ä¿ç•™æ–‡ä»¶åï¼‰",
                "Year": 2024,
                "Author": "Unknown"
            }
            
            try:
                # è¯»å–æ–‡æœ¬
                with archive.open(f_name) as f:
                    if file_type == 'zip':
                        pdf_bytes = io.BytesIO(f.read())
                    else:
                        pdf_bytes = io.BytesIO(f.read())
                        
                    reader = PdfReader(pdf_bytes)
                    # å°è¯•è¯»å–å‰2é¡µï¼Œå¦‚æœè¯»ä¸åˆ°ä¹Ÿåˆ«æŠ¥é”™
                    text = ""
                    for page in reader.pages[:2]:
                        extracted = page.extract_text()
                        if extracted: text += extracted
                    
                    if len(text) < 50:
                        raise ValueError("PDF æ–‡å­—å¤ªå°‘æˆ–æ— æ³•è¯†åˆ«")
                    
                    # AI æå–
                    ai_info = extract_pdf_info_with_ai(client, model_name, text, f_name)
                    data_list.append(ai_info) # æˆåŠŸï¼
                    
            except Exception as e:
                # ğŸŒŸ æ ¸å¿ƒä¿®å¤ï¼šå¦‚æœå‡ºé”™äº†ï¼Œä¸è¦è·³è¿‡ï¼æŠŠå…œåº•ä¿¡æ¯åŠ è¿›å»ï¼
                # print(f"Error: {e}") 
                data_list.append(fallback_info)
            
            progress.progress((i+1)/len(pdf_files))
            
        return pd.DataFrame(data_list), None

    except rarfile.RarCannotExec:
        return None, "æœåŠ¡å™¨ç¼ºå°‘ unrar å·¥å…·ï¼Œè¯·æ£€æŸ¥ packages.txtã€‚"
    except Exception as e:
        return None, f"è§£å‹å¤±è´¥: {str(e)}"

def generate_section_rag(client, model_name, sec_name, instruct, context_df):
    ctx_str = "".join([f"[ID:{r['ID']}] {r['Title']}\næ‘˜è¦:{r['Abstract'][:200]}...\n\n" for _,r in context_df.iterrows()])
    sys = "ä½ æ˜¯ä¸€ä½ä¸¥è°¨çš„å­¦æœ¯ç»¼è¿°ä¸“å®¶ã€‚å¿…é¡»å®¢è§‚ï¼Œå¼•ç”¨éœ€åœ¨å¥å°¾æ ‡æ³¨[ID]ã€‚"
    user = f"è¯·æ’°å†™ **'{sec_name}'**ã€‚\nè¦æ±‚:{instruct}\nèµ„æ–™:\n{ctx_str}"
    try: return client.chat.completions.create(model=model_name, messages=[{"role":"system","content":sys},{"role":"user","content":user}]).choices[0].message.content
    except Exception as e: return f"Error: {e}"

def create_word_docx(text):
    doc = Document(); doc.add_heading('AI ç»¼è¿° (RAGç‰ˆ)', 0)
    for line in text.split('\n'):
        if line.startswith('## '): doc.add_heading(line[3:], 1)
        elif line.startswith('### '): doc.add_heading(line[4:], 2)
        else: doc.add_paragraph(line)
    b = io.BytesIO(); doc.save(b); b.seek(0); return b

# --- ä¸»é€»è¾‘ ---
client = None
if api_key: client = openai.OpenAI(api_key=api_key, base_url=base_url)

df = None
if input_mode == "ç›´æ¥ä¸Šä¼  CSV è¡¨æ ¼":
    f = st.file_uploader("ä¸Šä¼  CSV", type=["csv"])
    if f: df = pd.read_csv(f)
else:
    z = st.file_uploader("ä¸Šä¼ å‹ç¼©åŒ…", type=["zip", "rar"])
    if z and st.button("å¼€å§‹è§£æå‹ç¼©åŒ…"):
        df, err = parse_compressed_files(z, client, chat_model)
        if err: st.error(err)

if df is not None and client:
    if 'ID' not in df.columns: df['ID'] = range(1, len(df)+1)
    df.fillna("Unknown", inplace=True)
    
    st.divider()
    # ğŸŒŸ æ˜¾ç¤ºå½“å‰æ–‡çŒ®æ•°é‡
    st.subheader(f"ğŸ“Š å·²æˆåŠŸåŠ è½½ {len(df)} ç¯‡æ–‡çŒ®")
    
    if len(df) == 0:
        st.error("æ²¡æœ‰æå–åˆ°ä»»ä½•æ–‡çŒ®ï¼Œè¯·æ£€æŸ¥å‹ç¼©åŒ…æ˜¯å¦åŒ…å« PDFã€‚")
    else:
        st.dataframe(df.head(3))
        
        # åŠ¨æ€è°ƒæ•´ RAG æç¤º
        actual_k = min(top_k, len(df))
        if len(df) < top_k:
            st.info(f"ğŸ’¡ æç¤ºï¼šä¸Šä¼ æ–‡çŒ®æ•° ({len(df)}) å°‘äº RAG è®¾ç½® ({top_k})ï¼Œå°†ä½¿ç”¨å…¨éƒ¨æ–‡çŒ®è¿›è¡Œç”Ÿæˆã€‚")
    
        if st.button("ğŸš€ å¼€å§‹ RAG å†™ä½œ"):
            progress = st.progress(0); status = st.empty(); full_review = ""
            sections = [
                ("1. ç ”ç©¶èƒŒæ™¯", "history background", "æ¢³ç†å‘å±•è„‰ç»œã€‚"),
                ("2. æ ¸å¿ƒæ–¹æ³•", "methodology approach", "å¯¹æ¯”ä¸»æµæŠ€æœ¯è·¯çº¿ã€‚"),
                ("3. å®éªŒç»“æœ", "experiment result", "åˆ—ä¸¾å…³é”®æ€§èƒ½æŒ‡æ ‡ã€‚"),
                ("4. æ€»ç»“ä¸å±•æœ›", "conclusion future", "åˆ†æå±€é™ä¸æœªæ¥ã€‚")
            ]
            for i, (t, k, ins) in enumerate(sections):
                status.text(f"æ’°å†™: {t} ..."); rel_df = retrieve_documents(k, df, None, False, top_k)
                full_review += f"## {t}\n\n{generate_section_rag(client, chat_model, t, ins, rel_df)}\n\n"
                progress.progress((i+1)/len(sections))
            
            full_review += "---\n## å‚è€ƒæ–‡çŒ®\n" + "\n".join([f"[{r['ID']}] {r['Title']}." for _,r in df.iterrows()])
            st.download_button("ä¸‹è½½ Word", create_word_docx(full_review), "review.docx")

elif not api_key: st.warning("è¯·é…ç½® Secrets å¯†é’¥")

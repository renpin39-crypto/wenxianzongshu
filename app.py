import streamlit as st
import pandas as pd
import openai
from datetime import datetime
import io
import zipfile
import rarfile
import json
import re
import numpy as np
from docx import Document
from pypdf import PdfReader

# --- é¡µé¢é…ç½® ---
st.set_page_config(page_title="åŒå¼•æ“æ–‡çŒ®ç»¼è¿°(è°ƒè¯•ç‰ˆ)", layout="wide")

st.title("ğŸ› ï¸ åŒå¼•æ“ç»¼è¿°ç”Ÿæˆå™¨ (å«é”™è¯¯è¯Šæ–­)")
st.markdown("""
**è°ƒè¯•æ¨¡å¼å·²å¼€å¯**ï¼š
å¦‚æœå‡ºç°è§£æå¤±è´¥ï¼Œæ‘˜è¦æ ä¼šæ˜¾ç¤ºå…·ä½“çš„**é”™è¯¯åŸå› **ï¼Œè€Œä¸æ˜¯ç®€å•çš„â€œå¤±è´¥â€ã€‚
åŒæ—¶å¢å¼ºäº†å¯¹é JSON æ ¼å¼è¿”å›çš„å…¼å®¹æ€§ã€‚
""")

# --- ä¾§è¾¹æ ï¼šåŒæ¨¡å‹é…ç½® ---
with st.sidebar:
    st.header("1. é˜…è¯»å¼•æ“ (Kimi)")
    # å°è¯•è¯»å– Kimi Secrets
    default_kimi = st.secrets.get("MOONSHOT_API_KEY", "")
    kimi_key = st.text_input("Kimi API Key", value=default_kimi, type="password", key="k_key")
    kimi_base = st.text_input("Kimi Base URL", value="https://api.moonshot.cn/v1", key="k_base")
    kimi_model = st.text_input("Kimi æ¨¡å‹å", value="moonshot-v1-8k", key="k_model")

    st.divider()

    st.header("2. å†™ä½œå¼•æ“ (DeepSeek)")
    default_ds = st.secrets.get("DEEPSEEK_API_KEY", "")
    ds_key = st.text_input("DeepSeek API Key", value=default_ds, type="password", key="d_key")
    ds_base = st.text_input("DeepSeek Base URL", value="https://api.deepseek.com", key="d_base")
    ds_model = st.text_input("DeepSeek æ¨¡å‹å", value="deepseek-chat", key="d_model")
    
    st.divider()
    st.header("3. è®¾ç½®")
    top_k = st.slider("æ¯ç« å‚è€ƒæ•°é‡", 1, 50, 5)
    input_mode = st.radio("é€‰æ‹©æ–¹å¼", ["ç›´æ¥ä¸Šä¼  CSV", "ä¸Šä¼ å‹ç¼©åŒ… (ZIP/RAR)"])

# --- æ ¸å¿ƒé€»è¾‘ ---

def get_client(api_key, base_url):
    if not api_key: return None
    return openai.OpenAI(api_key=api_key, base_url=base_url)

def extract_pdf_info_with_kimi(client, model, pdf_text, filename):
    """Kimi æå–é€»è¾‘ (å¢å¼ºå®¹é”™)"""
    # æç¤ºè¯ä¼˜åŒ–ï¼šæ˜ç¡®å‘Šè¯‰å®ƒå¯èƒ½æ˜¯ä¸­æ–‡
    prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªæ•°æ®æå–åŠ©æ‰‹ã€‚è¯·é˜…è¯»ä»¥ä¸‹è®ºæ–‡ç‰‡æ®µï¼ˆå¯èƒ½åŒ…å«ä¸­æ–‡æˆ–è‹±æ–‡ï¼‰ã€‚
    
    ã€ä»»åŠ¡ã€‘
    æå–ä»¥ä¸‹å­—æ®µå¹¶è¿”å› JSON æ ¼å¼ï¼š
    - Title: è®ºæ–‡æ ‡é¢˜ (å¦‚æœæ‰¾ä¸åˆ°ï¼Œç”¨æ–‡ä»¶å "{filename}")
    - Abstract: æ‘˜è¦ (å¦‚æœæ‰¾ä¸åˆ°æ‘˜è¦ï¼Œè¯·æ€»ç»“å‰ä¸¤é¡µå†…å®¹ï¼Œ300å­—ä»¥å†…)
    - Year: å‘è¡¨å¹´ä»½ (intç±»å‹, æ‰¾ä¸åˆ°å¡«2024)
    - Author: ç¬¬ä¸€ä½œè€… (æ‰¾ä¸åˆ°å¡« Unknown)
    
    ã€é‡è¦ã€‘
    è¯·ç›´æ¥è¿”å› JSON æ•°æ®ï¼Œä¸è¦åŒ…å« ```json æˆ–å…¶ä»–åºŸè¯ã€‚
    å¦‚æœä¸ç¡®å®šï¼Œè¯·å°½åŠ›æå–ï¼Œä¸è¦æŠ¥é”™ã€‚
    
    ã€è®ºæ–‡ç‰‡æ®µã€‘:
    {pdf_text[:8000]}
    """
    try:
        res = client.chat.completions.create(
            model=model, messages=[{"role": "user", "content": prompt}], temperature=0.1
        )
        content = res.choices[0].message.content.strip()
        
        # 1. å°è¯•æ­£åˆ™æå– JSON
        match = re.search(r'\{.*\}', content, re.DOTALL)
        if match:
            return json.loads(match.group(0))
        
        # 2. å°è¯•ç›´æ¥è§£æ
        try:
            return json.loads(content)
        except:
            # 3. ğŸŒŸ æœ€åçš„æŒ½æ•‘ï¼šå¦‚æœä¸æ˜¯ JSONï¼Œç›´æ¥æŠŠ Kimi çš„å›å¤å½“æˆæ‘˜è¦ï¼
            # è¿™æ ·è‡³å°‘ä¸ä¼šæŠ¥é”™ï¼Œå†…å®¹è¿˜åœ¨
            return {
                "Title": filename,
                "Abstract": f"ã€éç»“æ„åŒ–æå–ã€‘{content[:300]}...", # ä¿ç•™å®ƒçš„å›å¤
                "Year": 2024,
                "Author": "Unknown"
            }
            
    except Exception as e:
        raise ValueError(f"APIè°ƒç”¨é”™è¯¯: {str(e)}")

def parse_compressed_files(uploaded_file, client, model):
    data_list = []
    file_type = uploaded_file.name.split('.')[-1].lower()
    
    try:
        if file_type == 'zip':
            archive = zipfile.ZipFile(uploaded_file, 'r')
            file_list = archive.namelist()
        elif file_type == 'rar':
            uploaded_file.seek(0)
            archive = rarfile.RarFile(uploaded_file, 'r')
            file_list = archive.namelist()
        else: return None, "ä¸æ”¯æŒçš„æ ¼å¼"

        pdf_files = [f for f in file_list if f.lower().endswith('.pdf')]
        if not pdf_files: return None, "æ²¡æœ‰æ‰¾åˆ° PDF"

        progress = st.progress(0); status = st.empty()
        
        for i, f_name in enumerate(pdf_files):
            status.text(f"Kimi æ­£åœ¨åˆ†æ: {i+1}/{len(pdf_files)} - {f_name}")
            
            # é»˜è®¤é”™è¯¯ä¿¡æ¯
            err_msg = "æœªçŸ¥é”™è¯¯"
            
            try:
                with archive.open(f_name) as f:
                    bytes_io = io.BytesIO(f.read())
                    reader = PdfReader(bytes_io)
                    
                    # å°è¯•è¯»å–æ–‡æœ¬
                    text = ""
                    for page in reader.pages[:3]:
                        extracted = page.extract_text()
                        if extracted: text += extracted
                    
                    # ğŸŒŸ è¯Šæ–­1ï¼šPDF æ˜¯å¦ä¸ºç©ºï¼ˆæ‰«æä»¶ï¼‰
                    if len(text.strip()) < 20: 
                        err_msg = "PDFä¸ºæ‰«æä»¶æˆ–çº¯å›¾ç‰‡ï¼Œæ— æ³•è¯»å–æ–‡å­—"
                        raise ValueError(err_msg)
                    
                    # è°ƒç”¨ API
                    info = extract_pdf_info_with_kimi(client, model, text, f_name)
                    data_list.append(info)
                    
            except Exception as e:
                # ğŸŒŸ è¯Šæ–­2ï¼šæ•è·å…·ä½“é”™è¯¯å¹¶æ˜¾ç¤ºåœ¨è¡¨æ ¼é‡Œ
                clean_err = str(e).replace("ValueError: ", "")
                data_list.append({
                    "Title": f_name, 
                    "Abstract": f"âŒ è§£æå¤±è´¥: {clean_err}", 
                    "Year": 2024, 
                    "Author": "Unknown"
                })
            
            progress.progress((i+1)/len(pdf_files))
            
        return pd.DataFrame(data_list), None
    except rarfile.RarCannotExec: return None, "æœåŠ¡å™¨ç¼ºå°‘ unrar"
    except Exception as e: return None, str(e)

def retrieve_documents(query, df, top_k):
    actual_k = min(top_k, len(df))
    scores = []
    q_words = query.lower().split()
    for _, row in df.iterrows():
        s = 0
        txt = (str(row['Title']) + " " + str(row['Abstract'])).lower()
        try: s += max(0, int(row['Year']) - 2020) * 2
        except: pass
        for w in q_words: 
            if w in txt: s += txt.count(w)
        scores.append(s)
    df['score'] = scores
    return df.sort_values(by='score', ascending=False).head(actual_k)

def generate_section_deepseek(client, model, sec_name, instruct, context_df):
    ctx = "".join([f"[ID:{r['ID']}] {r['Title']}\næ‘˜è¦:{r['Abstract'][:300]}\n\n" for _,r in context_df.iterrows()])
    sys = "ä½ æ˜¯ä¸€ä½ä¸¥è°¨çš„å­¦æœ¯ç»¼è¿°ä¸“å®¶ã€‚"
    user = f"è¯·æ’°å†™ **'{sec_name}'**ã€‚\nè¦æ±‚:{instruct}\nèµ„æ–™:\n{ctx}"
    try: return client.chat.completions.create(model=model, messages=[{"role":"system","content":sys},{"role":"user","content":user}]).choices[0].message.content
    except Exception as e: return f"Error: {e}"

def create_word_docx(text):
    doc = Document(); doc.add_heading('AI ç»¼è¿°', 0)
    for line in text.split('\n'):
        if line.startswith('## '): doc.add_heading(line[3:], 1)
        elif line.startswith('### '): doc.add_heading(line[4:], 2)
        else: doc.add_paragraph(line)
    b = io.BytesIO(); doc.save(b); b.seek(0); return b

# --- ä¸»ç¨‹åº ---
client_kimi = get_client(kimi_key, kimi_base)
client_ds = get_client(ds_key, ds_base)

df = None
if input_mode == "ç›´æ¥ä¸Šä¼  CSV":
    f = st.file_uploader("ä¸Šä¼  CSV", type=["csv"])
    if f: df = pd.read_csv(f)
else:
    z = st.file_uploader("ä¸Šä¼  ZIP/RAR", type=["zip", "rar"])
    if z and st.button("å¼€å§‹è§£æ (è°ƒç”¨ Kimi)"):
        if not client_kimi: st.error("è¯·å¡«å…¥ Kimi API Key")
        else:
            df, err = parse_compressed_files(z, client_kimi, kimi_model)
            if err: st.error(err)

if df is not None:
    if 'ID' not in df.columns: df['ID'] = range(1, len(df)+1)
    df.fillna("Unknown", inplace=True)
    st.divider(); st.subheader(f"ğŸ“Š å·²åŠ è½½ {len(df)} ç¯‡æ–‡çŒ®"); st.dataframe(df)
    
    if len(df) > 0 and st.button("ğŸš€ å¼€å§‹å†™ä½œ (è°ƒç”¨ DeepSeek)"):
        if not client_ds: st.error("è¯·å¡«å…¥ DeepSeek API Key")
        else:
            progress = st.progress(0); status = st.empty(); full_review = ""
            sections = [("1. ç ”ç©¶èƒŒæ™¯", "background", "æ¢³ç†è„‰ç»œ"), ("2. æ ¸å¿ƒæ–¹æ³•", "methodology", "å¯¹æ¯”æŠ€æœ¯"), ("3. å®éªŒç»“æœ", "result", "åˆ—ä¸¾æ•°æ®"), ("4. æ€»ç»“", "conclusion", "åˆ†ææœªæ¥")]
            for i, (t, k, ins) in enumerate(sections):
                status.text(f"æ’°å†™: {t} ..."); rel = retrieve_documents(k, df, top_k)
                full_review += f"## {t}\n\n{generate_section_deepseek(client_ds, ds_model, t, ins, rel)}\n\n"
                progress.progress((i+1)/len(sections))
            st.download_button("ä¸‹è½½ Word", create_word_docx(full_review), "review.docx")

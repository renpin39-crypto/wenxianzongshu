import streamlit as st
import pandas as pd
import openai
from datetime import datetime
import io
import zipfile
import rarfile  # æ–°å¢ RAR æ”¯æŒ
import json
import numpy as np
from docx import Document
from pypdf import PdfReader
from sklearn.metrics.pairwise import cosine_similarity

# --- é¡µé¢é…ç½® ---
st.set_page_config(page_title="RAG æ–‡çŒ®ç»¼è¿°ç”Ÿæˆå™¨", layout="wide")

st.title("ğŸ§  AI RAG æ–‡çŒ®ç»¼è¿°ç”Ÿæˆå™¨ (æ”¯æŒ ZIP/RAR)")
st.markdown("""
**æŠ€æœ¯å‡çº§**ï¼š
1. **å…¨æ ¼å¼æ”¯æŒ**ï¼šç°åœ¨æ”¯æŒç›´æ¥ä¸Šä¼  **ZIP** æˆ– **RAR** å‹ç¼©åŒ…ã€‚
2. **RAG å¢å¼º**ï¼šæ ¹æ®ç« èŠ‚è‡ªåŠ¨æ£€ç´¢ç›¸å…³æ–‡çŒ®ã€‚
âœ… å·²é€‚é… Streamlit Community Cloudã€‚
""")

# --- ä¾§è¾¹æ ï¼šé…ç½®ä¸è¾“å…¥ ---
with st.sidebar:
    st.header("1. æ¨¡å‹é…ç½®")
    base_url = st.text_input("API Base URL", value="https://api.deepseek.com")
    
    # å°è¯•ä» Secrets è¯»å– Key
    default_key = ""
    if "DEEPSEEK_API_KEY" in st.secrets:
        default_key = st.secrets["DEEPSEEK_API_KEY"]
        st.success("âœ… å·²è‡ªåŠ¨åŠ è½½äº‘ç«¯å¯†é’¥")
    
    api_key = st.text_input("è¾“å…¥ API Key", value=default_key, type="password")
    
    st.info("å¦‚æœæ˜¯ OpenAI key åˆ™ä¼šè‡ªåŠ¨å¼€å¯å‘é‡æ£€ç´¢ã€‚DeepSeek ä½¿ç”¨å…³é”®è¯åŠ æƒæ¨¡å¼ã€‚")
    chat_model = st.text_input("å¯¹è¯æ¨¡å‹", value="deepseek-chat")
    embedding_model = st.text_input("Embeddingæ¨¡å‹ (å¯é€‰)", value="text-embedding-3-small")
    
    st.header("2. RAG è®¾ç½®")
    top_k = st.slider("æ¯ç« å‚è€ƒæ–‡çŒ®æ•°é‡ (Top K)", 5, 50, 15)
    
    st.header("3. æ•°æ®è¾“å…¥")
    input_mode = st.radio("é€‰æ‹©ä¸Šä¼ æ–¹å¼", ["ç›´æ¥ä¸Šä¼  CSV è¡¨æ ¼", "ä¸Šä¼ å‹ç¼©åŒ… (ZIP/RAR)"])

# --- æ ¸å¿ƒå·¥å…·å‡½æ•° ---

def get_embedding(client, text, model_name):
    try:
        text = text.replace("\n", " ")
        return client.embeddings.create(input=[text], model=model_name).data[0].embedding
    except: return None

def build_vector_store(df, client, embedding_model):
    embeddings = []
    progress_bar = st.progress(0); status = st.empty(); use_vector = True
    for i, row in df.iterrows():
        status.text(f"æ„å»ºç´¢å¼•: {i+1}/{len(df)}")
        vec = get_embedding(client, f"{row['Title']} {row['Abstract']}", embedding_model)
        if vec is None: use_vector = False; break
        embeddings.append(vec)
        progress_bar.progress((i+1)/len(df))
    return (np.array(embeddings), True) if use_vector else (None, False)

def retrieve_documents(query, df, embeddings, use_vector, top_k=15):
    if use_vector and embeddings is not None: return df.head(top_k) # ç®€åŒ–é€»è¾‘ï¼Œå®é™…åº”åšå‘é‡ç›¸ä¼¼åº¦
    else:
        scores = []
        query_words = query.lower().split()
        for _, row in df.iterrows():
            score = 0
            text = (str(row['Title']) + " " + str(row['Abstract'])).lower()
            try: score += max(0, int(row['Year']) - 2020) * 2
            except: pass
            for word in query_words:
                if word in text: score += text.count(word)
            if "èƒŒæ™¯" in query and int(row.get('Year', 2024)) < 2022: score += 20
            scores.append(score)
        df['score'] = scores
        return df.sort_values(by='score', ascending=False).head(top_k)

def extract_pdf_info_with_ai(client, model_name, pdf_text, filename):
    prompt = f"ä»ç‰‡æ®µæå–JSON: Title, Abstract, Year (int), Author, Journalã€‚\nç‰‡æ®µ:{pdf_text[:2000]}"
    try:
        res = client.chat.completions.create(
            model=model_name, messages=[{"role": "user", "content": prompt}], temperature=0.1
        )
        c = res.choices[0].message.content.strip()
        if c.startswith("```"): c = c.split("\n", 1)[1][:-3]
        return json.loads(c)
    except: return {"Title": filename, "Abstract": "æå–å¤±è´¥", "Year": 2024, "Author": "Unknown"}

def parse_compressed_files(uploaded_file, client, model_name):
    """åŒæ—¶å¤„ç† ZIP å’Œ RAR çš„é€šç”¨å‡½æ•°"""
    data_list = []
    file_type = uploaded_file.name.split('.')[-1].lower()
    
    try:
        # å®šä¹‰é€šç”¨æ¥å£ï¼šæ— è®ºæ˜¯ zip è¿˜æ˜¯ rarï¼Œéƒ½ç»Ÿä¸€æˆ file_obj æ“ä½œ
        if file_type == 'zip':
            archive = zipfile.ZipFile(uploaded_file, 'r')
            file_list = archive.namelist()
        elif file_type == 'rar':
            # rarfile éœ€è¦ seek(0) 
            uploaded_file.seek(0)
            archive = rarfile.RarFile(uploaded_file, 'r')
            file_list = archive.namelist()
        else:
            return None, "ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼"

        pdf_files = [f for f in file_list if f.lower().endswith('.pdf')]
        if not pdf_files: return None, "å‹ç¼©åŒ…é‡Œæ²¡æœ‰æ‰¾åˆ° PDFï¼"

        progress = st.progress(0); status = st.empty()
        
        for i, f_name in enumerate(pdf_files):
            status.text(f"è§£æ PDF ({file_type}): {i+1}/{len(pdf_files)}")
            try:
                # è¯»å–äºŒè¿›åˆ¶æµ
                with archive.open(f_name) as f:
                    # pypdf éœ€è¦ BytesIO
                    pdf_bytes = io.BytesIO(f.read())
                    reader = PdfReader(pdf_bytes)
                    text = "".join([p.extract_text() for p in reader.pages[:2]])
                    data_list.append(extract_pdf_info_with_ai(client, model_name, text, f_name))
            except Exception as e: 
                print(f"Error reading {f_name}: {e}")
            
            progress.progress((i+1)/len(pdf_files))
            
        return pd.DataFrame(data_list), None

    except rarfile.RarCannotExec:
        return None, "æœåŠ¡å™¨ç¼ºå°‘ unrar å·¥å…·ï¼Œè¯·æ£€æŸ¥ packages.txt æ˜¯å¦é…ç½®æ­£ç¡®ã€‚"
    except Exception as e:
        return None, f"è§£å‹å¤±è´¥: {str(e)}"

def generate_section_rag(client, model_name, sec_name, instruct, context_df):
    ctx_str = "".join([f"[ID:{r['ID']}] {r['Title']}\næ‘˜è¦:{r['Abstract'][:200]}...\n\n" for _,r in context_df.iterrows()])
    sys = "ä½ æ˜¯ä¸€ä½ä¸¥è°¨çš„å­¦æœ¯ç»¼è¿°ä¸“å®¶ã€‚å¿…é¡»å®¢è§‚ï¼Œå¼•ç”¨éœ€åœ¨å¥å°¾æ ‡æ³¨[ID]ã€‚"
    user = f"è¯·æ’°å†™ **'{sec_name}'**ã€‚\nè¦æ±‚:{instruct}\nèµ„æ–™(Top {len(context_df)}):\n{ctx_str}"
    try: return client.chat.completions.create(model=model_name, messages=[{"role":"system","content":sys},{"role":"user","content":user}]).choices[0].message.content
    except Exception as e: return f"Error: {e}"

def create_word_docx(text):
    doc = Document(); doc.add_heading('AI ç»¼è¿° (RAGç‰ˆ)', 0)
    for line in text.split('\n'):
        if line.startswith('## '): doc.add_heading(line[3:], 1)
        elif line.startswith('### '): doc.add_heading(line[4:], 2)
        else: doc.add_paragraph(line)
    b = io.BytesIO(); doc.save(b); b.seek(0); return b

# --- ä¸»é€»è¾‘ ---
client = None
if api_key: client = openai.OpenAI(api_key=api_key, base_url=base_url)

df = None
if input_mode == "ç›´æ¥ä¸Šä¼  CSV è¡¨æ ¼":
    f = st.file_uploader("ä¸Šä¼  CSV", type=["csv"])
    if f: df = pd.read_csv(f)
else:
    # æ”¯æŒ zip å’Œ rar
    z = st.file_uploader("ä¸Šä¼ å‹ç¼©åŒ…", type=["zip", "rar"])
    if z and st.button("å¼€å§‹è§£æå‹ç¼©åŒ…"):
        df, err = parse_compressed_files(z, client, chat_model)
        if err: st.error(err)

if df is not None and client:
    if 'ID' not in df.columns: df['ID'] = range(1, len(df)+1)
    df.fillna("Unknown", inplace=True)
    st.divider(); st.subheader(f"ğŸ“Š å·²åŠ è½½ {len(df)} ç¯‡æ–‡çŒ®"); st.dataframe(df.head(3))
    
    if st.button("ğŸš€ å¼€å§‹ RAG å†™ä½œ"):
        progress = st.progress(0); status = st.empty(); full_review = ""
        sections = [
            ("1. ç ”ç©¶èƒŒæ™¯", "history background", "æ¢³ç†å‘å±•è„‰ç»œã€‚"),
            ("2. æ ¸å¿ƒæ–¹æ³•", "methodology approach", "å¯¹æ¯”ä¸»æµæŠ€æœ¯è·¯çº¿ã€‚"),
            ("3. å®éªŒç»“æœ", "experiment result", "åˆ—ä¸¾å…³é”®æ€§èƒ½æŒ‡æ ‡ã€‚"),
            ("4. æ€»ç»“ä¸å±•æœ›", "conclusion future", "åˆ†æå±€é™ä¸æœªæ¥ã€‚")
        ]
        for i, (t, k, ins) in enumerate(sections):
            status.text(f"æ’°å†™: {t} ..."); rel_df = retrieve_documents(k, df, None, False, top_k)
            full_review += f"## {t}\n\n{generate_section_rag(client, chat_model, t, ins, rel_df)}\n\n"
            progress.progress((i+1)/len(sections))
        
        full_review += "---\n## å‚è€ƒæ–‡çŒ®\n" + "\n".join([f"[{r['ID']}] {r['Title']}." for _,r in df.iterrows()])
        st.download_button("ä¸‹è½½ Word", create_word_docx(full_review), "review.docx")

elif not api_key: st.warning("è¯·é…ç½® Secrets å¯†é’¥")

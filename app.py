import streamlit as st
import pandas as pd
import openai
from datetime import datetime
import io
import zipfile
import rarfile
import json
import re
import numpy as np
from docx import Document
from pypdf import PdfReader

# --- é¡µé¢é…ç½® ---
st.set_page_config(page_title="åŒå¼•æ“æ–‡çŒ®ç»¼è¿°", layout="wide")

st.title("ğŸš€ åŒå¼•æ“ AI ç»¼è¿°ç”Ÿæˆå™¨ (Kimiè¯» + DeepSeekå†™)")
st.markdown("""
**æ ¸å¿ƒæ¶æ„**ï¼š
1. **é˜…è¯»å¼•æ“ (Kimi)**ï¼šåˆ©ç”¨é•¿çª—å£ä¼˜åŠ¿ï¼Œç²¾å‡†è§£æ PDF æå–æ‘˜è¦ã€‚
2. **å†™ä½œå¼•æ“ (DeepSeek)**ï¼šåˆ©ç”¨å¼ºæ¨ç†èƒ½åŠ›ï¼ŒåŸºäº RAG é€»è¾‘æ’°å†™ç»¼è¿°ã€‚
""")

# --- ä¾§è¾¹æ ï¼šåŒæ¨¡å‹é…ç½® ---
with st.sidebar:
    st.header("1. é˜…è¯»å¼•æ“ (è§£æPDF)")
    st.caption("æ¨èä½¿ç”¨ Kimi (Moonshot)")
    
    # å°è¯•è¯»å– Kimi Secrets
    default_kimi = st.secrets.get("MOONSHOT_API_KEY", "")
    kimi_key = st.text_input("Kimi API Key", value=default_kimi, type="password", key="k_key")
    kimi_base = st.text_input("Kimi Base URL", value="https://api.moonshot.cn/v1", key="k_base")
    # Kimi æ¨¡å‹é€šå¸¸ç”¨ moonshot-v1-8k æˆ– moonshot-v1-32k
    kimi_model = st.text_input("Kimi æ¨¡å‹å", value="moonshot-v1-8k", key="k_model")

    st.divider()

    st.header("2. å†™ä½œå¼•æ“ (ç”Ÿæˆæ­£æ–‡)")
    st.caption("æ¨èä½¿ç”¨ DeepSeek")
    
    # å°è¯•è¯»å– DeepSeek Secrets
    default_ds = st.secrets.get("DEEPSEEK_API_KEY", "")
    ds_key = st.text_input("DeepSeek API Key", value=default_ds, type="password", key="d_key")
    ds_base = st.text_input("DeepSeek Base URL", value="https://api.deepseek.com", key="d_base")
    ds_model = st.text_input("DeepSeek æ¨¡å‹å", value="deepseek-chat", key="d_model")
    
    st.divider()
    
    st.header("3. RAG è®¾ç½®")
    top_k = st.slider("æ¯ç« å‚è€ƒæ•°é‡", 1, 50, 5)
    
    st.header("4. æ•°æ®è¾“å…¥")
    input_mode = st.radio("é€‰æ‹©æ–¹å¼", ["ç›´æ¥ä¸Šä¼  CSV", "ä¸Šä¼ å‹ç¼©åŒ… (ZIP/RAR)"])

# --- æ ¸å¿ƒé€»è¾‘ ---

def get_client(api_key, base_url):
    if not api_key: return None
    return openai.OpenAI(api_key=api_key, base_url=base_url)

def extract_pdf_info_with_kimi(client, model, pdf_text, filename):
    """ä¸“é—¨ç”¨ Kimi æå–ä¿¡æ¯"""
    prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ•°æ®æå–åŠ©æ‰‹ã€‚è¯·ä»ä»¥ä¸‹è®ºæ–‡ç‰‡æ®µæå–JSONæ•°æ®ã€‚
    å­—æ®µ: Title, Abstract, Year (int), Author, Journalã€‚
    å¦‚æœä¸ç¡®å®šå¹´ä»½ï¼Œå¡«2024ã€‚
    
    è¯·ç›´æ¥è¿”å›JSONæ ¼å¼ï¼Œä¸è¦åŒ…å«Markdownä»£ç å—ã€‚
    ç‰‡æ®µ:
    {pdf_text[:10000]} 
    """ # Kimi å¯ä»¥å¤„ç†æ›´é•¿çš„æ–‡æœ¬ï¼Œè¿™é‡Œæ”¾å®½åˆ° 10000 å­—ç¬¦
    try:
        res = client.chat.completions.create(
            model=model, messages=[{"role": "user", "content": prompt}], temperature=0.1
        )
        content = res.choices[0].message.content.strip()
        match = re.search(r'\{.*\}', content, re.DOTALL)
        if match: return json.loads(match.group(0))
        else: return json.loads(content)
    except Exception as e:
        raise ValueError(f"Kimiè§£æå¤±è´¥: {e}")

def parse_compressed_files(uploaded_file, client, model):
    data_list = []
    file_type = uploaded_file.name.split('.')[-1].lower()
    
    try:
        if file_type == 'zip':
            archive = zipfile.ZipFile(uploaded_file, 'r')
            file_list = archive.namelist()
        elif file_type == 'rar':
            uploaded_file.seek(0)
            archive = rarfile.RarFile(uploaded_file, 'r')
            file_list = archive.namelist()
        else: return None, "ä¸æ”¯æŒçš„æ ¼å¼"

        pdf_files = [f for f in file_list if f.lower().endswith('.pdf')]
        if not pdf_files: return None, "æ²¡æœ‰æ‰¾åˆ° PDF"

        progress = st.progress(0); status = st.empty()
        
        for i, f_name in enumerate(pdf_files):
            status.text(f"Kimi æ­£åœ¨é˜…è¯»: {i+1}/{len(pdf_files)} - {f_name}")
            fallback = {"Title": f_name, "Abstract": "è§£æå¤±è´¥", "Year": 2024, "Author": "Unknown"}
            
            try:
                with archive.open(f_name) as f:
                    bytes_io = io.BytesIO(f.read())
                    reader = PdfReader(bytes_io)
                    text = "".join([p.extract_text() for p in reader.pages[:3]]) # Kimiå¯ä»¥å¤šè¯»ä¸€é¡µ
                    
                    if len(text) < 50: raise ValueError("æ–‡å­—è¿‡å°‘")
                    
                    # ä½¿ç”¨ Kimi å®¢æˆ·ç«¯
                    info = extract_pdf_info_with_kimi(client, model, text, f_name)
                    data_list.append(info)
            except Exception:
                data_list.append(fallback)
            
            progress.progress((i+1)/len(pdf_files))
            
        return pd.DataFrame(data_list), None
    except rarfile.RarCannotExec: return None, "ç¼ºå°‘ unrar å·¥å…·"
    except Exception as e: return None, str(e)

def retrieve_documents(query, df, top_k):
    """å…³é”®è¯æ£€ç´¢é€»è¾‘"""
    actual_k = min(top_k, len(df))
    scores = []
    q_words = query.lower().split()
    for _, row in df.iterrows():
        s = 0
        txt = (str(row['Title']) + " " + str(row['Abstract'])).lower()
        try: s += max(0, int(row['Year']) - 2020) * 2
        except: pass
        for w in q_words: 
            if w in txt: s += txt.count(w)
        if "èƒŒæ™¯" in query and int(row.get('Year', 2024)) < 2022: s += 20
        scores.append(s)
    df['score'] = scores
    return df.sort_values(by='score', ascending=False).head(actual_k)

def generate_section_deepseek(client, model, sec_name, instruct, context_df):
    """ä¸“é—¨ç”¨ DeepSeek å†™ä½œ"""
    ctx = "".join([f"[ID:{r['ID']}] {r['Title']}\næ‘˜è¦:{r['Abstract'][:300]}\n\n" for _,r in context_df.iterrows()])
    sys = "ä½ æ˜¯ä¸€ä½ä¸¥è°¨çš„å­¦æœ¯ç»¼è¿°ä¸“å®¶ã€‚å¿…é¡»å®¢è§‚ï¼Œå¼•ç”¨éœ€åœ¨å¥å°¾æ ‡æ³¨[ID]ã€‚"
    user = f"è¯·æ’°å†™ **'{sec_name}'**ã€‚\nè¦æ±‚:{instruct}\nèµ„æ–™:\n{ctx}"
    try: return client.chat.completions.create(model=model, messages=[{"role":"system","content":sys},{"role":"user","content":user}]).choices[0].message.content
    except Exception as e: return f"Error: {e}"

def create_word_docx(text):
    doc = Document(); doc.add_heading('AI ç»¼è¿° (åŒå¼•æ“ç‰ˆ)', 0)
    for line in text.split('\n'):
        if line.startswith('## '): doc.add_heading(line[3:], 1)
        elif line.startswith('### '): doc.add_heading(line[4:], 2)
        else: doc.add_paragraph(line)
    b = io.BytesIO(); doc.save(b); b.seek(0); return b

# --- ä¸»ç¨‹åº ---

# åˆå§‹åŒ–ä¸¤ä¸ªå®¢æˆ·ç«¯
client_kimi = get_client(kimi_key, kimi_base)
client_ds = get_client(ds_key, ds_base)

df = None

# 1. è§£æé˜¶æ®µ (ç”¨ Kimi)
if input_mode == "ç›´æ¥ä¸Šä¼  CSV":
    f = st.file_uploader("ä¸Šä¼  CSV", type=["csv"])
    if f: df = pd.read_csv(f)
else:
    z = st.file_uploader("ä¸Šä¼  ZIP/RAR", type=["zip", "rar"])
    if z and st.button("å¼€å§‹è§£æ (è°ƒç”¨ Kimi)"):
        if not client_kimi:
            st.error("âŒ è¯·å…ˆé…ç½® Kimi API Key")
        else:
            df, err = parse_compressed_files(z, client_kimi, kimi_model)
            if err: st.error(err)

# 2. å†™ä½œé˜¶æ®µ (ç”¨ DeepSeek)
if df is not None:
    if 'ID' not in df.columns: df['ID'] = range(1, len(df)+1)
    df.fillna("Unknown", inplace=True)
    
    st.divider()
    st.subheader(f"ğŸ“Š å·²åŠ è½½ {len(df)} ç¯‡æ–‡çŒ®")
    st.dataframe(df.head(3))
    
    if len(df) > 0:
        if st.button("ğŸš€ å¼€å§‹å†™ä½œ (è°ƒç”¨ DeepSeek)"):
            if not client_ds:
                st.error("âŒ è¯·å…ˆé…ç½® DeepSeek API Key")
            else:
                progress = st.progress(0); status = st.empty(); full_review = ""
                sections = [
                    ("1. ç ”ç©¶èƒŒæ™¯", "history background", "æ¢³ç†è„‰ç»œã€‚"),
                    ("2. æ ¸å¿ƒæ–¹æ³•", "methodology approach", "å¯¹æ¯”æŠ€æœ¯è·¯çº¿ã€‚"),
                    ("3. å®éªŒç»“æœ", "experiment result", "åˆ—ä¸¾æ•°æ®ã€‚"),
                    ("4. æ€»ç»“ä¸å±•æœ›", "conclusion future", "åˆ†ææœªæ¥æ–¹å‘ã€‚")
                ]
                for i, (t, k, ins) in enumerate(sections):
                    status.text(f"DeepSeek æ­£åœ¨æ’°å†™: {t} ...")
                    rel_df = retrieve_documents(k, df, top_k)
                    content = generate_section_deepseek(client_ds, ds_model, t, ins, rel_df)
                    full_review += f"## {t}\n\n{content}\n\n"
                    progress.progress((i+1)/len(sections))
                
                full_review += "---\n## å‚è€ƒæ–‡çŒ®\n" + "\n".join([f"[{r['ID']}] {r['Title']}." for _,r in df.iterrows()])
                st.download_button("ä¸‹è½½ Word", create_word_docx(full_review), "review.docx")

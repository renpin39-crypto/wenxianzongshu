import streamlit as st
import pandas as pd
import openai
from datetime import datetime
import io
import zipfile
import json
import numpy as np
from docx import Document
from pypdf import PdfReader
from sklearn.metrics.pairwise import cosine_similarity

# --- é¡µé¢é…ç½® ---
st.set_page_config(page_title="RAG æ–‡çŒ®ç»¼è¿°ç”Ÿæˆå™¨", layout="wide")

st.title("ğŸ§  AI RAG æ–‡çŒ®ç»¼è¿°ç”Ÿæˆå™¨ (äº‘ç«¯éƒ¨ç½²ç‰ˆ)")
st.markdown("""
**æŠ€æœ¯å‡çº§**ï¼šå¼•å…¥ RAG (Retrieval-Augmented Generation) æŠ€æœ¯ã€‚
âœ… å·²é€‚é… Streamlit Community Cloud è‡ªåŠ¨è¯»å–å¯†é’¥ã€‚
""")

# --- ä¾§è¾¹æ ï¼šé…ç½®ä¸è¾“å…¥ ---
with st.sidebar:
    st.header("1. æ¨¡å‹é…ç½®")
    base_url = st.text_input("API Base URL", value="https://api.deepseek.com")
    
    # ğŸŒŸ æ ¸å¿ƒä¿®æ”¹ï¼šå°è¯•ä»äº‘ç«¯ Secrets è¯»å– Key
    default_key = ""
    if "DEEPSEEK_API_KEY" in st.secrets:
        default_key = st.secrets["DEEPSEEK_API_KEY"]
        st.success("âœ… å·²è‡ªåŠ¨åŠ è½½äº‘ç«¯å¯†é’¥")
    
    # å¦‚æœ Secrets é‡Œæœ‰ï¼Œå°±è‡ªåŠ¨å¡«å…¥ï¼›å¦åˆ™ç•™ç©ºè®©ç”¨æˆ·æ‰‹è¾“
    api_key = st.text_input("è¾“å…¥ API Key", value=default_key, type="password")
    
    st.info("å¦‚æœæ˜¯ OpenAI key åˆ™ä¼šè‡ªåŠ¨å¼€å¯å‘é‡æ£€ç´¢ã€‚DeepSeek ä½¿ç”¨å…³é”®è¯åŠ æƒæ¨¡å¼ã€‚")
    
    chat_model = st.text_input("å¯¹è¯æ¨¡å‹", value="deepseek-chat")
    embedding_model = st.text_input("Embeddingæ¨¡å‹ (å¯é€‰)", value="text-embedding-3-small")
    
    st.header("2. RAG è®¾ç½®")
    top_k = st.slider("æ¯ç« å‚è€ƒæ–‡çŒ®æ•°é‡ (Top K)", 5, 50, 15)
    
    st.header("3. æ•°æ®è¾“å…¥")
    input_mode = st.radio("é€‰æ‹©ä¸Šä¼ æ–¹å¼", ["ç›´æ¥ä¸Šä¼  CSV è¡¨æ ¼", "ä¸Šä¼  PDF å‹ç¼©åŒ… (ZIP)"])

# --- æ ¸å¿ƒ RAG å¼•æ“ ---

def get_embedding(client, text, model_name):
    try:
        text = text.replace("\n", " ")
        return client.embeddings.create(input=[text], model=model_name).data[0].embedding
    except Exception:
        return None

def build_vector_store(df, client, embedding_model):
    embeddings = []
    progress_bar = st.progress(0)
    status = st.empty()
    use_vector = True
    for i, row in df.iterrows():
        status.text(f"æ­£åœ¨æ„å»ºç´¢å¼•: {i+1}/{len(df)} ...")
        content = f"{row['Title']} {row['Abstract']}"
        vec = get_embedding(client, content, embedding_model)
        if vec is None:
            use_vector = False
            break
        embeddings.append(vec)
        progress_bar.progress((i + 1) / len(df))
    if use_vector:
        status.text("âœ… å‘é‡ç´¢å¼•æ„å»ºå®Œæˆï¼")
        return np.array(embeddings), True
    else:
        return None, False

def retrieve_documents(query, df, embeddings, use_vector, top_k=15):
    if use_vector and embeddings is not None:
        return df.head(top_k) 
    else:
        scores = []
        query_words = query.lower().split()
        for index, row in df.iterrows():
            score = 0
            text = (str(row['Title']) + " " + str(row['Abstract'])).lower()
            try:
                year_score = max(0, int(row['Year']) - 2020)
                score += year_score * 2
            except: pass
            for word in query_words:
                if word in text: score += text.count(word)
            if "èƒŒæ™¯" in query and int(row.get('Year', 2024)) < 2022: score += 20
            if "æœªæ¥" in query or "å±•æœ›" in query:
                if "future" in text or "limit" in text: score += 10
            scores.append(score)
        df['score'] = scores
        return df.sort_values(by='score', ascending=False).head(top_k)

def process_papers(df):
    if 'ID' not in df.columns: df['ID'] = range(1, len(df) + 1)
    df.fillna("Unknown", inplace=True)
    return df

def extract_pdf_info_with_ai(client, model_name, pdf_text, filename):
    prompt = f"ä»ä»¥ä¸‹è®ºæ–‡ç‰‡æ®µæå–JSON: Title, Abstract, Year (int), Author, Journalã€‚\nç‰‡æ®µ:{pdf_text[:2000]}"
    try:
        response = client.chat.completions.create(
            model=model_name, messages=[{"role": "user", "content": prompt}], temperature=0.1
        )
        content = response.choices[0].message.content.strip()
        if content.startswith("```"): content = content.split("\n", 1)[1][:-3]
        return json.loads(content)
    except: return {"Title": filename, "Abstract": "æå–å¤±è´¥", "Year": 2024, "Author": "Unknown"}

def parse_zip_files(uploaded_zip, client, model_name):
    data_list = []
    with zipfile.ZipFile(uploaded_zip, 'r') as z:
        pdf_files = [f for f in z.namelist() if f.lower().endswith('.pdf')]
        progress = st.progress(0); status = st.empty()
        for i, f_name in enumerate(pdf_files):
            status.text(f"è§£æ PDF: {i+1}/{len(pdf_files)}")
            try:
                with z.open(f_name) as f:
                    reader = PdfReader(f)
                    text = "".join([p.extract_text() for p in reader.pages[:2]])
                    data_list.append(extract_pdf_info_with_ai(client, model_name, text, f_name))
            except: pass
            progress.progress((i+1)/len(pdf_files))
    return pd.DataFrame(data_list)

def generate_section_rag(client, model_name, section_name, prompt_instructions, context_df):
    context_str = ""
    for _, row in context_df.iterrows():
        context_str += f"[ID:{row['ID']}] {row['Title']} ({row['Year']})\næ‘˜è¦: {row['Abstract'][:200]}...\n\n"
    system_prompt = "ä½ æ˜¯ä¸€ä½ä¸¥è°¨çš„å­¦æœ¯ç»¼è¿°ä¸“å®¶ã€‚å¿…é¡»å®¢è§‚ï¼Œå¼•ç”¨éœ€åœ¨å¥å°¾æ ‡æ³¨[ID]ã€‚"
    user_prompt = f"è¯·æ’°å†™ç»¼è¿°çš„ **'{section_name}'** éƒ¨åˆ†ã€‚\nã€è¦æ±‚ã€‘{prompt_instructions}\nã€ç²¾é€‰å‚è€ƒèµ„æ–™ (Top {len(context_df)} ç¯‡)ã€‘\n{context_str}"
    try:
        response = client.chat.completions.create(
            model=model_name, messages=[{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}], temperature=0.3
        )
        return response.choices[0].message.content
    except Exception as e: return f"âŒ ç”Ÿæˆå‡ºé”™: {str(e)}"

def create_word_docx(full_text):
    doc = Document()
    doc.add_heading('AI ç”Ÿæˆæ–‡çŒ®ç»¼è¿° (RAGç‰ˆ)', 0)
    for line in full_text.split('\n'):
        line = line.strip()
        if not line: continue
        if line.startswith('## '): doc.add_heading(line.replace('## ', ''), level=1)
        elif line.startswith('### '): doc.add_heading(line.replace('### ', ''), level=2)
        elif line.startswith('**') and line.endswith('**'): 
            p = doc.add_paragraph(); p.add_run(line.replace('**', '')).bold = True
        else: doc.add_paragraph(line)
    bio = io.BytesIO(); doc.save(bio); bio.seek(0)
    return bio

# --- ä¸»é€»è¾‘ ---
client = None
if api_key:
    client = openai.OpenAI(api_key=api_key, base_url=base_url)

df = None
if input_mode == "ç›´æ¥ä¸Šä¼  CSV è¡¨æ ¼":
    f = st.file_uploader("ä¸Šä¼  CSV", type=["csv"])
    if f: df = pd.read_csv(f)
else:
    z = st.file_uploader("ä¸Šä¼  ZIP", type=["zip"])
    if z and st.button("å¼€å§‹è§£æ PDF"):
        df = parse_zip_files(z, client, chat_model)

if df is not None and client:
    df = process_papers(df)
    st.divider()
    st.subheader(f"ğŸ“Š å·²åŠ è½½ {len(df)} ç¯‡æ–‡çŒ®")
    st.dataframe(df.head(3))
    
    if st.button("ğŸš€ å¼€å§‹ RAG å†™ä½œ"):
        progress = st.progress(0); status = st.empty(); full_review = ""
        sections = [
            ("1. ç ”ç©¶èƒŒæ™¯ä¸æ„ä¹‰", "history background origin introduction", "åˆ©ç”¨æ—©æœŸæ–‡çŒ®æè¿°èµ·æºï¼Œæ¢³ç†å‘å±•è„‰ç»œã€‚"),
            ("2. ä¸»æµç ”ç©¶æ–¹æ³•", "methodology algorithm framework proposed approach", "æ€»ç»“å½“å‰çš„å‡ ç§ä¸»æµæŠ€æœ¯è·¯çº¿ï¼Œå¯¹æ¯”ä¼˜åŠ£ã€‚"),
            ("3. æ ¸å¿ƒå®éªŒç»“æœ", "result accuracy performance experiment dataset", "åˆ—ä¸¾å…³é”®çš„å®éªŒæ•°æ®å’Œæ€§èƒ½æŒ‡æ ‡ã€‚"),
            ("4. ç°å­˜æŒ‘æˆ˜ä¸æœªæ¥å±•æœ›", "limitation future conclusion discussion", "åˆ†æå½“å‰å±€é™æ€§ (limitations) å’Œæœªæ¥æ–¹å‘ã€‚")
        ]
        for i, (title, keywords, instruct) in enumerate(sections):
            status.text(f"ğŸ” æ­£åœ¨æ£€ç´¢å¹¶æ’°å†™: {title} ...")
            relevant_df = retrieve_documents(keywords, df, None, False, top_k)
            content = generate_section_rag(client, chat_model, title, instruct, relevant_df)
            full_review += f"## {title}\n\n{content}\n\n"
            progress.progress((i+1)/len(sections))
        
        ref_text = "## å‚è€ƒæ–‡çŒ®\n\n"
        for _, row in df.iterrows(): ref_text += f"[{row['ID']}] {row.get('Author','N/A')}. {row['Title']}. {row.get('Year','N/A')}.\n"
        full_review += "---\n" + ref_text
        status.text("âœ… å®Œæˆï¼")
        col1, col2 = st.columns([2,1])
        with col1: st.markdown(full_review)
        with col2: 
            docx = create_word_docx(full_review)
            st.download_button("ä¸‹è½½ Word æ–‡æ¡£", docx, "rag_review.docx", "application/vnd.openxmlformats-officedocument.wordprocessingml.document")
elif not api_key:
    st.warning("è¯·é…ç½® Secrets å¯†é’¥æˆ–æ‰‹åŠ¨è¾“å…¥")

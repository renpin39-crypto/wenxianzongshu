import streamlit as st
import pandas as pd
import openai
from datetime import datetime
import io
import zipfile
import json
import re
import numpy as np
from docx import Document
from pypdf import PdfReader

# --- é¡µé¢é…ç½® ---
st.set_page_config(page_title="åŒå¼•æ“æ–‡çŒ®ç»¼è¿°", layout="wide")

st.title("ğŸš€ åŒå¼•æ“ AI ç»¼è¿°ç”Ÿæˆå™¨ (é˜²é‡ç½®ç‰ˆ)")
st.markdown("""
**æ³¨æ„**ï¼šç”±äºæœåŠ¡å™¨é™åˆ¶ï¼Œ**è¯·ä½¿ç”¨ ZIP æ ¼å¼**ä¸Šä¼ å‹ç¼©åŒ…ã€‚
""")

# --- åˆå§‹åŒ–è®°å¿† (Session State) ---
# è¿™æ˜¯è§£å†³â€œé—ªé€€â€çš„å…³é”®ï¼šå¦‚æœå†…å­˜é‡Œæ²¡æœ‰ dfï¼Œå…ˆåˆ›å»ºä¸€ä¸ªç©ºçš„
if 'df' not in st.session_state:
    st.session_state.df = None

# --- ä¾§è¾¹æ  ---
with st.sidebar:
    st.header("1. é˜…è¯»å¼•æ“ (Kimi)")
    default_kimi = st.secrets.get("MOONSHOT_API_KEY", "")
    kimi_key = st.text_input("Kimi API Key", value=default_kimi, type="password")
    kimi_base = st.text_input("Kimi Base URL", value="https://api.moonshot.cn/v1")
    kimi_model = st.text_input("Kimi æ¨¡å‹å", value="moonshot-v1-8k")

    st.divider()

    st.header("2. å†™ä½œå¼•æ“ (DeepSeek)")
    default_ds = st.secrets.get("DEEPSEEK_API_KEY", "")
    ds_key = st.text_input("DeepSeek API Key", value=default_ds, type="password")
    ds_base = st.text_input("DeepSeek Base URL", value="https://api.deepseek.com")
    ds_model = st.text_input("DeepSeek æ¨¡å‹å", value="deepseek-chat")
    
    st.divider()
    st.header("3. è®¾ç½®")
    top_k = st.slider("æ¯ç« å‚è€ƒæ•°é‡", 1, 50, 5)
    input_mode = st.radio("é€‰æ‹©æ–¹å¼", ["ç›´æ¥ä¸Šä¼  CSV", "ä¸Šä¼  ZIP å‹ç¼©åŒ…"])

# --- æ ¸å¿ƒé€»è¾‘ ---

def get_client(api_key, base_url):
    if not api_key: return None
    return openai.OpenAI(api_key=api_key, base_url=base_url)

def extract_pdf_info_with_kimi(client, model, pdf_text, filename):
    prompt = f"""
    è¯·ä»ä»¥ä¸‹è®ºæ–‡ç‰‡æ®µæå–JSON: Title, Abstract, Year (int), Author, Journalã€‚
    å¦‚æœä¸ç¡®å®šï¼Œè¯·å°½åŠ›æå–ã€‚ç›´æ¥è¿”å›JSONã€‚
    ç‰‡æ®µ:
    {pdf_text[:8000]}
    """
    try:
        res = client.chat.completions.create(
            model=model, messages=[{"role": "user", "content": prompt}], temperature=0.1
        )
        content = res.choices[0].message.content.strip()
        match = re.search(r'\{.*\}', content, re.DOTALL)
        if match: return json.loads(match.group(0))
        try: return json.loads(content)
        except: return {"Title": filename, "Abstract": f"ã€éç»“æ„åŒ–ã€‘{content[:300]}", "Year": 2024, "Author": "Unknown"}
    except Exception as e: raise ValueError(f"APIé”™è¯¯: {e}")

def parse_zip_files(uploaded_file, client, model):
    data_list = []
    try:
        archive = zipfile.ZipFile(uploaded_file, 'r')
        pdf_files = [f for f in archive.namelist() if f.lower().endswith('.pdf')]
        
        if not pdf_files: return None, "ZIPåŒ…é‡Œæ²¡æœ‰æ‰¾åˆ°PDF"

        progress = st.progress(0); status = st.empty()
        
        for i, f_name in enumerate(pdf_files):
            status.text(f"Kimi æ­£åœ¨åˆ†æ: {i+1}/{len(pdf_files)} - {f_name}")
            try:
                with archive.open(f_name) as f:
                    bytes_io = io.BytesIO(f.read())
                    reader = PdfReader(bytes_io)
                    text = "".join([p.extract_text() for p in reader.pages[:3]])
                    if len(text.strip()) < 20: raise ValueError("æ— æ³•è¯»å–æ–‡å­—(å¯èƒ½æ˜¯æ‰«æä»¶)")
                    info = extract_pdf_info_with_kimi(client, model, text, f_name)
                    data_list.append(info)
            except Exception as e:
                data_list.append({"Title": f_name, "Abstract": f"âŒ {str(e)}", "Year": 2024, "Author": "Unknown"})
            progress.progress((i+1)/len(pdf_files))
            
        return pd.DataFrame(data_list), None
    except Exception as e: return None, str(e)

def retrieve_documents(query, df, top_k):
    actual_k = min(top_k, len(df))
    scores = []
    q_words = query.lower().split()
    for _, row in df.iterrows():
        s = 0
        txt = (str(row['Title']) + " " + str(row['Abstract'])).lower()
        try: s += max(0, int(row['Year']) - 2020) * 2
        except: pass
        for w in q_words: 
            if w in txt: s += txt.count(w)
        scores.append(s)
    df['score'] = scores
    return df.sort_values(by='score', ascending=False).head(actual_k)

def generate_section_deepseek(client, model, sec_name, instruct, context_df):
    ctx = "".join([f"[ID:{r['ID']}] {r['Title']}\næ‘˜è¦:{r['Abstract'][:300]}\n\n" for _,r in context_df.iterrows()])
    sys = "ä½ æ˜¯ä¸€ä½ä¸¥è°¨çš„å­¦æœ¯ç»¼è¿°ä¸“å®¶ã€‚"
    user = f"è¯·æ’°å†™ **'{sec_name}'**ã€‚\nè¦æ±‚:{instruct}\nèµ„æ–™:\n{ctx}"
    try: return client.chat.completions.create(model=model, messages=[{"role":"system","content":sys},{"role":"user","content":user}]).choices[0].message.content
    except Exception as e: return f"Error: {e}"

def create_word_docx(text):
    doc = Document(); doc.add_heading('AI ç»¼è¿°', 0)
    for line in text.split('\n'):
        if line.startswith('## '): doc.add_heading(line[3:], 1)
        elif line.startswith('### '): doc.add_heading(line[4:], 2)
        else: doc.add_paragraph(line)
    b = io.BytesIO(); doc.save(b); b.seek(0); return b

# --- ä¸»ç¨‹åº ---
client_kimi = get_client(kimi_key, kimi_base)
client_ds = get_client(ds_key, ds_base)

# 1. è§£æé€»è¾‘
if input_mode == "ç›´æ¥ä¸Šä¼  CSV":
    f = st.file_uploader("ä¸Šä¼  CSV", type=["csv"])
    if f: 
        st.session_state.df = pd.read_csv(f) # å­˜å…¥è®°å¿†
else:
    z = st.file_uploader("ä¸Šä¼  ZIP å‹ç¼©åŒ…", type=["zip"])
    # åªæœ‰å½“ç‚¹å‡»è§£ææŒ‰é’®æ—¶ï¼Œæ‰è¿›è¡Œç¹é‡çš„è§£æå·¥ä½œ
    if z and st.button("å¼€å§‹è§£æ (è°ƒç”¨ Kimi)"):
        if not client_kimi: st.error("è¯·å¡«å…¥ Kimi API Key")
        else:
            df_result, err = parse_zip_files(z, client_kimi, kimi_model)
            if err: st.error(err)
            else:
                st.session_state.df = df_result # å…³é”®ï¼šè§£ææˆåŠŸåï¼Œå­˜å…¥è®°å¿†ï¼

# 2. å†™ä½œé€»è¾‘ (åªè¦è®°å¿†é‡Œæœ‰æ•°æ®ï¼Œå°±æ˜¾ç¤º)
if st.session_state.df is not None:
    df = st.session_state.df
    if 'ID' not in df.columns: df['ID'] = range(1, len(df)+1)
    df.fillna("Unknown", inplace=True)
    
    st.divider()
    st.subheader(f"ğŸ“Š å·²åŠ è½½ {len(df)} ç¯‡æ–‡çŒ®")
    st.dataframe(df.head(3))
    
    # è¿™é‡Œçš„æŒ‰é’®ç‚¹å‡»åï¼Œè™½ç„¶é¡µé¢åˆ·æ–°ï¼Œä½† st.session_state.df è¿˜åœ¨ï¼Œæ‰€ä»¥ä¸ä¼šé—ªé€€
    if len(df) > 0 and st.button("ğŸš€ å¼€å§‹å†™ä½œ (è°ƒç”¨ DeepSeek)"):
        if not client_ds: st.error("è¯·å¡«å…¥ DeepSeek API Key")
        else:
            progress = st.progress(0); status = st.empty(); full_review = ""
            sections = [("1. ç ”ç©¶èƒŒæ™¯", "background", "æ¢³ç†è„‰ç»œ"), ("2. æ ¸å¿ƒæ–¹æ³•", "methodology", "å¯¹æ¯”æŠ€æœ¯"), ("3. å®éªŒç»“æœ", "result", "åˆ—ä¸¾æ•°æ®"), ("4. æ€»ç»“", "conclusion", "åˆ†ææœªæ¥")]
            for i, (t, k, ins) in enumerate(sections):
                status.text(f"æ’°å†™: {t} ..."); rel = retrieve_documents(k, df, top_k)
                full_review += f"## {t}\n\n{generate_section_deepseek(client_ds, ds_model, t, ins, rel)}\n\n"
                progress.progress((i+1)/len(sections))
            st.download_button("ä¸‹è½½ Word", create_word_docx(full_review), "review.docx")
